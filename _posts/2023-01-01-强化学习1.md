---
title: 【强化学习】马尔可夫决策过程
tags: 强化学习 笔记
published: false
---

Markov Decision Process(MDP)是一个四元组 $<S,A,P,R>$：

- $S$ 是状态集合：在$t$时刻的状态记为 $S_t$

- $A$ 是行为集合：在$t$时刻的行为记为 $A_t$，有时候行为集合与状态有关，记为 $A(s)$

- $P$ 是状态转移概率分布：$P(S_{t+1}=s'\|S_t=s,A_t=a)$，表示在状态 $s$ 下选择行为 $a$ 后，转移到 $s'$ 的概率，简记为 $P(s'\|s,a)$

- $R$ 是奖励函数：$R:S\times A\times S→\mathbb{R}$，记 $R(s,a,s')$ 为在状态 $s$ 下选择行为$a$到达状态 $s'$ 的奖励。在 $t$ 时刻的奖励用 $r_t$ 表示


## 策略

策略是智能体在状态 $s$ 下选择行为$a$的概率，记为 $\pi(a\|s)$。如果是确定性的策略，则用 $a=\pi(s)$表示。

<div align=center>
<img src="../../../assets/images/posts/misc/rl.png" width="20%" />
<p style="font-style: italic; color: gray; margin-top: 0.5em;">
</p>
</div>

如上图所示，与监督学习从数据中学习不同，强化学习通过与环境交互获得反馈进行学习，而MDP用数学建模了这个与环境交互的过程。具体来说，Agent处在一个状态 $s$，根据自己的策略 $\pi$ 做出行为 $a$ 后，就会得到环境反馈的奖励 $r$，并根据状态转移概率分布 $P$ 到达新的状态 $s'$。写成代码就是：

```python
done = False
state = env.reset()
while not done:
    action = agent.take_step(state)
    state_, reward, done = env.step(action)
    agent.learn(state_, reward)  # 强化学习
    state = state_
```

强化学习是要学习一个让长期奖励最大化的策略。

## 值函数

强化学习的目标是最大化长期的奖励，因此我们需要定义一个能代表长期奖励的量进行优化，通常采用的是**_回报（return）_**，定义为：

$$
G_t=r_{t+1}+\gamma r_{t+2}+\cdots
$$

$\gamma\in[0, 1]$ 称为 **_折扣因子_**，如果$\gamma$更接近1，表示 $G_t$ 更关注长期利益，反之更短视。通常 $\gamma$ 取0.9，0.99之类的值。

在强化学习中，目前有两大类学习策略的方法：

- 一种是直接学习一个函数作为策略，叫**_策略梯度法_**，使用神经网络表示策略 $$\pi_{\theta}(\cdot\|s)$$，然后用梯度下降来训练神经网络，这种方比较直接，但是很多其中涉及到的很多思想来源于下面的方法，因此这个后面再说。

- 一种是间接的学习策略，是基于值函数的方法，具体来说是定义两个函数：
    - **_状态值函数_**：$$V^{\pi}(s)=E[G_t\|S_t=s]$$，表示从状态 $s$ 开始，回报的期望，也叫V函数
    - **_行为值函数_**：$$Q^{\pi}(s,a)=E[G_t\|S_t=s,A_t=a]$$，表示在状态 $s$ 做出行为 $a$ 后，回报的期望，也叫Q函数

    注意<span style="color: red">值函数是依赖于策略的</span>，上标为 $\pi$ 表示在策略 $\pi$ 下的值函数。有了Q函数之后可以通过求解 $\text{argmax}_a Q(s,a)$ 来获取行为，也就得到了一个基于Q的策略。

基于值函数的方法是基础，所以先介绍后者。

### 值函数性质

对于给定的策略，两个值函数的关系为（省略了上标 $\pi$）：

$$
\begin{aligned}

V(s)
=& E_{a\sim \pi(\cdot|s)}[Q(s,a)]
\\

Q(s,a)
=& E_{s'\sim p(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]

\end{aligned} 
$$

定义 $P^{\pi}(\cdot\|s)=\int_a P(\cdot\|s,a)\pi(a\|s)$，这样就变成一个纯粹的状态转移分布。对于离散状态和动作空间的MDP，V函数是个向量，Q函数是矩阵，$P^{\pi}$ 也是个矩阵，就可以得到矩阵形式的方程：

$$
\begin{aligned}
v &= P^{\pi}Q = r + \gamma P^{\pi}v \\
Q &= P^{\pi}[r + \gamma v]
\end{aligned}
$$

给定一个策略，可以通过求解第一个式子得到V函数，然后用第二个式子求出Q函数。$v=T^{\pi}(v)$，$T^{\pi}$ 叫贝尔曼算子。可以看出来这个其实是求解不动点，在 $T^{\pi}$ 满足一定条件的情况下，任意初始化 $v$ 都可以收敛于 $v^{\pi}$。

### 最优值函数与最优策略

- 最优值函数：
    - 最优状态值函数：$V^*(s)=\max_{\pi}V^{\pi}(s)$
    - 最优行为值函数：$Q^*(s)=\max_{\pi}Q^{\pi}(s,a)$

    这里的最优是在所有状态下最优，也就是帕累托最优。一个反直觉的地方是觉得两个策略，可能有的在这个状态更好，有的在那个状态更好。下面说明会最优值函数存在。

- 定义偏序关系$\pi≥\pi'$为$\forall s\in S(V^{\pi}(s)≥V^{\pi'}(s))$

    根据前面$V$和$Q$的关系，容易得到如果$\pi≥\pi'$，也有$\forall s\in S,\forall a\in A(Q^{\pi}(s,a)≥Q^{\pi'}(s,a))$。注意这里是采取相同的$a$的情况下。反过来也正确。

定理：对任意的MDP，以下性质成立：

- $\exists\pi_\*\forall\pi(\pi_\*\ge\pi)$

    最优策略总是存在（但不一定唯一）

- $V^\*(s)=V^{\pi_\*}(s)$

    最优策略的状态值函数就是最优状态值函数。所以给定最优策略，$V^{\pi^*}(s)$在所有状态都是最优的。

- $Q^\*(s,a)=Q^{\pi_\*}(s,a)$

    最优策略的行为值函数就是最优行为值函数。

最优策略可能不唯一，但是最优值函数唯一，强化学习的目标就是找到这个最优策略。


定理：若$\forall s\in S$，取$\pi'(s)=\text{argmax}_a Q^{\pi}(s,a)$，则$\pi'≥\pi$，并且等号成立当且仅当$Q^{\pi}=Q^*$
{:.info}

对任意的状态$s$：

* 如果存在 $a,a'\in A(s)$，$Q(s,a)$ 和 $Q(s,a')$ 不一样，总是可以选择 $$\text{argmax}_a Q(s,a)$$ 使得 $V(s)$ 变大。这是因为 $ V(s)=E_{ a \sim \pi(\cdot \| s)}[Q(s,a)]< \max_a Q(s,a) $，基于这种选择就得到了更好的策略，因为$\pi'>\pi$

* 如果对任意的$a,a'\in A(s)$，有$Q(s,a)=Q(s,a')$，说明在这个状态下随便选哪个行为都一样的。


定理：$\pi(s)=\text{argmax}_a Q^*(s,a)$是最优策略。
{:.info}

对任意的策略$\pi'$，由于$Q^*(s,a)≥Q^{\pi'}(s,a)$，所以$\pi≥\pi'$，所以$\pi$是最优策略。

最优策略虽然不唯一，但是最优值函数唯一，我们可以通过找到最优值函数得到这样一个最优策略。

## Bellman方程

由：

$$
\begin{aligned}
V(s)
=&E_{a\sim \pi(\cdot|s)}[Q(s,a)]

\\
Q(s,a)
=& E_{s'\sim p(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]

\end{aligned} 
$$

容易得到：

$$
\begin{aligned}
V(s)&=
E_{a\sim \pi(\cdot|s)}[E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]]
\\
Q(s,a)&=
E_{s'\sim p(\cdot|s,a)}[E_{a'\sim \pi(\cdot|s')}[R(s,a,s')+\gamma Q(s',a')]]
\end{aligned} 
$$

这个叫Bellman方程（两个方程都是），如果已知策略，可以利用该方程求解$V$和$Q$。

Bellman方程是针对一个策略而言的。

### 策略迭代

策略迭代是求解Bellman方程的算法，不过需要状态空间和动作空间是有限的。

- 初始化$V(s)=0$，初始化策略 $\pi(a\|s)=\frac{1}{\|A(s)\|}$
- 迭代$N$次：
    - 策略估计：

        求$V_{\pi}$，重复计算以下公式即可：

        $$
        V(s)\leftarrow E_{a\sim \pi(\cdot|s)}[E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]]
        $$

            一个直观的理解是，$V(s)$ 初始为0，迭代一次就是计算了 $E[r_{t+1}\|S_t=s]$，两次则是 $E[r_{t+1}+\gamma r_{t+2}\|S_t=s]$，计算无穷次就是 $E[G_t\|S_t=s]$ 了。

    - 策略改进：

        $$
        \begin{aligned}
        Q(s,a)\leftarrow &E_{s'\sim P(\cdot|s,a)}[R(s,a)+\gamma V(s')]
        \\
        \pi(s) \leftarrow &\text{argmax}_{a}Q(s,a)
        \end{aligned} 
        $$

            前面证明了这样可以得到一个更好的策略，直到得到最优策略才不会变得更好，不过实践中就迭代$N$次。

不一定要求$V$，求$Q$也行，哪个方便求哪个。

## Bellman最优性方程

考虑最优策略的Bellman方程。

如果有$Q^*$以及最优策略$\pi^*(s)=\text{argmax}_a Q^*(s,a)$，可以得到$V^*(s)=\max_a Q^*(s,a)$，于是Bellman方程变为以下形式：

$$
V^*(s)
=
\max_a
 E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V^*(s')]

\\

Q^*(s,a)=
E_{s'\sim P(\cdot|s,a)}[
R(s,a,s')+\gamma \max_a Q^*(s',a)
]
$$

该方程称为Bellman最优性方程。

也可以写成$v=T^*(v)$。

### 值迭代

值迭代是求解Bellman最优性方程的算法，比策略迭代收敛更快（有时候）。

- $V(s)=0$
- 迭代$N$次：

$$
V(s)\leftarrow \max_aE_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]
$$

    这个其实就是动态规划，和算法课学的有些差别，因为$V$的定义不一样。
- $Q(s,a)\leftarrow  E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]$
- $\pi(s)\leftarrow \text{argmax}_aQ(s,a)$

- 从另一个角度理解值迭代：
  - 策略迭代收敛慢是因为需要迭代很多次进行策略评估，实际上如果策略评估只迭代一次就是值迭代了
  - 如果策略估计只迭代一次，那么策略迭代的每次迭代就是进行以下三步：

$$
\begin{aligned}
V(s)\leftarrow &
E_{a\sim \pi(\cdot|s)}[E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]]
\\
Q(s,a)\leftarrow &
E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]
\\
\pi(s)\leftarrow &
\text{argmax}_{a}Q(s,a)
\end{aligned}  
$$

      为了说明更清晰，给它加上角标$t$：

$$
\begin{aligned}
V_{t+1}(s)\leftarrow &
E_{a\sim \pi_t(\cdot|s)}[E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V_t(s')]]
\\
Q_{t+1}(s,a)\leftarrow &
E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V_{t+1}(s')]
\\
\pi_{t+1}(s)\leftarrow &
\text{argmax}_{a}Q_{t+1}(s,a)
\end{aligned}  
$$

      对其中第一行进行推导：

$$
\begin{aligned}
V_{t+1}(s)\leftarrow &
E_{a\sim \pi_t(\cdot|s)}[E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V_t(s')]]\\
=& E_{a\sim\pi_t(\cdot|s)}[Q_t(s,a)]\\
=& \max_a Q_{t}(s,a)\\
=& \max_a E_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V_t(s')]

\end{aligned}  
$$

      就得到了最终的值迭代公式：

$$
V(s)\leftarrow \max_aE_{s'\sim P(\cdot|s,a)}[R(s,a,s')+\gamma V(s')]
$$

      也就是值迭代。

