---
title: 【强化学习】深度强化学习
tags: 强化学习 深度学习 笔记
published: true
---

深度强化学习算法分为两个流派，**值函数近似**流派和**策略梯度**流派：

1. ​值函数近似​​流派的思想是，用一个神经网络来直接学习状态或状态-动作对的价值（Q函数或V函数）。学成之后，智能体只需选择价值最高的动作即可（即 argmax操作）。例如AlphaGo，其早期版本就属于这一思路，它用神经网络来预测每一步棋的胜率（即Q值）。

2. 策略梯度​​流派则更为直接，它让神经网络绕过价值评估，直接学习动作的概率分布（即策略）。

尽管当今的研究主流已偏向于策略梯度方法（如PPO、SAC等），但许多先进的策略梯度算法内部依然会用到值函数近似来辅助训练。因此，本文先介绍值函数近似的一些算法，再介绍策略梯度流派的算法。

<!--more-->

## 值函数近似

值函数近似流派和之前Model Free的方式类似，**使用神经网络近似Q函数或V函数**，基于Q函数得到策略。典型的代表是围棋AI，使用神经网络预测每种下法的胜率（这就是Q函数），并取argmax得到策略。

### DQN

[DQN(2013)](https://arxiv.org/abs/1312.5602)是值函数近似流派的代表性算法，使用CNN近似Q函数来玩游戏。输入是图像和不同的action，输出是回报。


#### DQN的算法思路

DQN的核心想法很简单：用神经网络求解Bellman最优方程。已知Bellman最优方程为：

$$
Q(s,a)=E_{s'\sim p(\cdot|s,a)}[r+\gamma\ \max\limits_{a'}Q(s',a')]
$$

要用神经网络近似 $Q$，使得对任意的 $s,a,r,s'$ 上述方程都能近似成立，可以使用MSE损失函数：

$$
\mathcal{L} = \frac1B\sum_{i=1}^B \left[ Q_{\theta}(s_i, a_i) - \underbrace{(r_i + \gamma \max_{a'} Q_{\theta}(s'_i, a'))}_{\text{“TD Target”}} \right]^2 

$$

这相当于训练一个回归模型，它的训练目标是和Q-Learning一样的TD Target。特别地，当 $s'$ 是结束状态时（在 $s$ 采取 $a$ 后如果游戏结束会返回下一个状态 $s'$，$s'$ 是结束状态，但没有实际意义），$Q(s,a)=r$ 是准确的值（不这样DQN可能不收敛）。综合来看，训练目标为：

$$
\text{target}=
\begin{cases}
r & s'是结束状态\\
r+\gamma\max\limits_a Q_{\theta}(s',a) & s'不是结束状态
\end{cases}
 
$$

于是目标函数变为：

$$
J(\theta)=\frac1B\sum\limits_{i=1}^B
[Q_{\theta}(s_i,a_i) - \text{target}_i]^2
$$

就是一个非常简单的loss，整体思想也很简单，不过实际使用的时候会遇到两个问题：

第一个问题是**训练数据从哪来？**如果使用现有策略**连续采样**产生的 $B$ 个数据作为一个batch对神经网络训练会导致神经网络严重的过拟合，因为样本之间不独立，相关性太高，类似于流式训练，模型无法拟合整个状态空间上的分布。为了解决这个问题，DQN提出**_经验回放_**策略，使用比较大的replay buffer保存走过的 $<s,a,r,s'>$（保证样本近似独立同分布，避免过拟合），每次从中采样一个batch的样本来近似期望$E_{\pi}$（replay buffer和batch都是越大越好）。相当于在玩游戏的过程中把之前的数据存起来。值得注意的是引入replay buffer的前提是这个算法是Off Policy的，而DQN是Off Policy的。

第二个问题是**训练目标 $r+\gamma \max\limits_a Q_{\theta}(s',a)$ 会随着模型参数不断变化，导致训练及其不稳定**。DQN的解决办法是将网络拷贝一份并保持参数不变来构建目标，这个拷贝的网络称为**_目标网络_**，记为 $Q_{\theta'}$，此时训练目标为：

$$

\text{target}=
\begin{cases}
r & s'是结束状态\\
r+\gamma\max\limits_a Q_{\textcolor{red}{\theta'}}(s',a) & s'不是结束状态
\end{cases}

$$

其中 $\theta'$ 是目标网络的被固定住的参数，每迭代一定的次数，就令 $\theta'\leftarrow \theta$ 更新目标网络。熟悉视觉自监督的读者可能会发现这个和视觉自监督的方法很类似，也是用不参与训练的模型给自己当teacher，类似于[meanTeacher](https://arxiv.org/abs/1703.01780)那种做法。但是这里为什么不用移动平均更新 $\theta'$ 呢？单纯是因为DQN提出太早，后来的方法都改用移动平均了。

#### 具体算法

1. 初始化一个最大长度为 $K$ 的队列 $q$ 作为replay buffer，
1. 初始化 $Q_{\theta}$，拷贝 $Q_{\theta}$ 得到 $Q_{\theta'}$
1. 基于 $Q_{\theta}$ 的 $\epsilon$-greedy策略进行探索，将 $<s,a,r,s'>$ 添加到对列中，直到对列装满
1. 迭代 $N$ 次：
    * 从replay buffer中均匀采样 $B$ 个样本，对每个样本计算 $\text{target}$
    * 计算损失函数，使用优化算法更新一次参数
    * 基于 $Q_{\theta}$ 探索一步（$\epsilon$-greedy），将 $<s,a,r,s'>$ 添加到对列中，并把队头的数据丢掉
    * 每迭代 $C$ 次，$\theta'\leftarrow \theta$

#### DQN的优缺点

DQN的优点是off-policy的，不需要用自己的策略收集数据，只要有数据就能训练，这也是replay buffer能work的基础。除此之外，DQN提出的2个方法：固定target网络以及replay buffer非常有用，后面的很多算法都有使用。DQN的缺点是继承了Q-Learning的高估问题，且只能解决离散动作空间的问题，而且很玄学，深度学习本身就比较玄学，深度强化学习更是玄学中的玄学，需要很仔细的调参才能work。

### DQN的一些改进

* [Double DQN(2015)](https://arxiv.org/abs/1509.06461)把Double Q-Learning的算法迁移过来解决高估问题。因为DQN本身就有一个额外的目标网络，刚好很契合Double Q-Learning的需求，所以不需要额外再训练一个神经网络。Double DQN就是把DQN的TD Target改成了 $r + \gamma Q_{\theta'}(s', \text{argmax} Q(s,a))$。
* [Prioritized replay(2015)](https://arxiv.org/abs/1511.05952)改进了从replay buffer采样batch的方式，原本是均匀采样，现在是按优先级采样，基于优先级更高、更重要的数据训练收敛更快且效果更好。那么优先级从哪来的？论文中把TD Error加上一个常量作为优先级，还会加上一个常量进行平滑。除此之外，为了保证每个数据都至少被采样一次，新数据会直接被加入到batch中。除此之外，还有通过重要性采样调整每个样本的loss权重以修正非均匀采样带来的偏差。
* [Dueling DQN(2015)](https://arxiv.org/abs/1511.06581)修改了Q函数神经网络的设计，这里涉及**_优势函数_** $A(s, a)=Q(s,a)-V(s)$ 的概念，表示某个动作相较于平均而言能带来多大的优势，可以用来建模一个action对长期回报的影响。基于这个公式，Dueling DQN把 $Q_{\theta}$ 建模为 $V_{\theta}(s)+[A_{\phi}(s,a)-\frac1{\|\mathcal A\|}\sum_a A_{\phi}(s,a)]$，显示地分开建模“状态好坏”和“动作好坏”。考虑到 $V_{\theta}$ 和 $A_{\phi}$ 都有状态 $s$ 的输入，可以让它们两个共享编码 $s$ 的网络。这里优势函数会减去Batch内不同action价值的均值应该是有偏的，但是也能work。
* [NoisyNet(2017)](https://arxiv.org/pdf/1706.10295)把神经网络的参数建模为随机的，例如假设参数服从多元高斯分布 $\mathcal N(w, \sigma)$，既要学均值，也要学方差。参数是随机，那么神经网络近似的Q函数就是随机的，对应的策略也是随机的，因而可以加强探索、改进模型效果。需要注意的点是不能在每次forward都重新采样模型参数，在模型的两次更新之间不应该重新采样模型参数，使得采样一条轨迹的策略是一致的。
* [Rainbow DQN(2017)](https://arxiv.org/abs/1710.02298)缝合了DQN的各种改进，包括上面的4个，还有TD(1)改为TD(n)等等。如果想要使用值函数流派的算法解决问题可以使用Rainbow DQN。




## 策略梯度法

策略梯度方法是一个和值函数近似很不同的流派，直接用神经网络 $\pi_{\theta}$ 来近似策略，思路更直接更好理解。策略梯度法的核心是**策略梯度定理**，它在Richard Sutton 1999年的REINFORCE算法论文中提出，下面进行介绍。

### REINFORCE算法

#### 策略梯度定理

如果想要优化策略，我们需要有一个目标函数，那么策略梯度的目标函数是什么？在强化学习中，这个问题的答案并不唯一，如果粗略的划分可以分为两类，这两类都挺常见：

1. 基于状态的目标函数

    $$
    \mathcal L = E_{s\sim d}[V(s)]
    $$

    其中 $d(s)$ 是 $s$ 的某种分布，不同的分布代表不同的目标。如果起始状态是唯一的 $s_0$，那么我们可以只最大化 $V(s_0)$，例如玩Atari游戏的场景；如果初始状态不唯一，那么目标是最大化 $E_{s\sim \mu} [V(s)]$，其中 $\mu(s)$ 是初始状态分布；还有更复杂的情况，即 $d$ 是依赖于策略的，例如取整个过程的平稳分布，此时要通过梯度方法优化这个目标函数会很困难，因为梯度不仅和 $V(s)$ 有关，还和复杂的分布 $d$ 有关，此时一般会忽略对 $d$ 的梯度，用一个有偏差的梯度去更新模型。不过我个人感觉这种情况没有意义，因为rollout总是从初始策略开始的。

2. 基于轨迹的目标函数

    除了前面介绍的策略梯度，还有一个版本的策略梯度很常见，它是最大化轨迹上的价值函数，例如这个经典的强化学习[博客](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/)还有李宏毅老师的课程都是基于这一类价值函数去讲的。用给定的策略 $\pi_{\theta}$ 在环境中探索能够得到轨迹 $\tau$，这个轨迹是一个随机变量因为它取决于环境的状态转移以及策略这两个随机因素。因此不妨把轨迹服从的分布简单记为 $\tau\sim \pi_{\theta}$（这种写法可能不是很严谨，但是大家都这么写）。然后我们基于 $\tau$ 定义目标函数去最大化轨迹上的价值：

    $$
    \mathcal L = E_{\tau \sim \pi_{\theta}}[\frac1T\sum_{t=0}^{T} Q^{\pi_{\theta}}(s_t, a_t)]
    $$

    其中 $s_t,a_t$ 是轨迹上的状态和行为。这个定理本质上和第一类基于状态的目标函数是一样的，但它看起来更简单、没有分各种情况，这是因为轨迹 $\tau$ 的分布既和初始状态分布 $\mu$ 有关，也和当前策略、状态转移有关，相当于把复杂的东西都扔进轨迹 $\tau$ 里面了。

无论是哪种目标函数，我们都需要求它的梯度来训练模型 $\pi_{\theta}$。这两类目标函数的梯度不太相同。状态类目标函数虽然有多种情况，但是它们的梯度都有类似的形式，[策略梯度定理(1999)](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)给出了这些目标函数的梯度：

$$
\begin{aligned}
\nabla \mathcal L
& \approx
\frac1{1-\gamma}
E_{s \sim d^{\pi_{\theta}}(\cdot)} \left[
    E_{a\sim \pi_{\theta}(\cdot|s)}[Q^{\pi_{\theta}}(s,a)\nabla\ln\pi_{\theta}(a|s)]
\right]
\\
& \propto E_{s,a\sim \pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla\ln\pi_{\theta}(a|s)]

\end{aligned}
$$

其中 $d^{\pi_{\theta}}$ 是一个和策略有关的状态分布，表示按策略 $\pi_{\theta}$ 从初始状态开始游走的状态分布。$s,a\sim\pi_{\theta}$ 表示 $s,a$ 都是用策略 $\pi_{\theta}$ 随机游走采样得到的。这个公式<span style="color: red; font-weight: bold;">在一些情况下是等于，在一些情况下是约等于（大多数情况）</span>，具体可以参考[《强化学习的数学原理》](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning/blob/main/3%20-%20Chapter%209%20Policy%20Gradient%20Methods.pdf)。但无论如何，我们都可以用策略梯度定理给出的梯度值去优化模型，只是可能需要调调学习率。

直观来看，$Q^{\pi_{\theta}}(s,a)\nabla \ln\pi_{\theta}(a\|s)$ 就是由Q函数加权的交叉熵损失，标签是智能体采取的action，在 $Q$ 比较大就鼓励模型分类出这个动作，否则打压这个动作。
{:.info}

轨迹类目标函数的梯度是：

$$
\nabla\mathcal L = E_{\tau \sim \pi_{\theta}}[
    \frac1T\sum_{t=0}^{T} Q^{\pi_{\theta}}(s_t, a_t) \nabla\ln\pi_{\theta}(a_t|s_t)
]
$$

这个推导很简单，所以先介绍轨迹类目标函数的策略梯度定理怎么推导，再给出状态类策略梯度定理的证明。

策略梯度定理的意义在于，它右边的期望项可以通过采样来近似。我们不需要求关于 $d$ 的梯度，只需要用现有策略探索得到轨迹 $<s_0,a_0,r_1,s_1,\cdots>$，并用轨迹中的 $s$ 和 $a$ 计算 $Q^{\pi_{\theta}}(s,a)\nabla \ln\pi_{\theta}(a\|s)$ 并取平均即可。

#### SFGE梯度估计算法

SFGE的全称是Score Function Gradient Estimate，用来求 $\nabla_{\theta} E_{p_{\theta}(x)}[f(x)]$，在两类策略梯度的证明中都有用它的思路。它的思路很简单：

$$
\begin{aligned}
\nabla_{\theta} E_{p_{\theta}(x)}[f(x)]
&=\nabla_{\theta} \int f(x)p_{\theta}(x)dx\\
&=\int f(x)  \textcolor{red}{\nabla_{\theta} p_{\theta}(x)}  dx\\
&=\int f(x)  \textcolor{red}{p_{\theta}(x)\nabla_{\theta}\ln p_{\theta}(x)} dx\\
&=E_{p_{\theta}(x)}[f(x)\nabla_{\theta}\ln p_{\theta}(x)]
\end{aligned} 
$$

公式里面红色的部分叫**_Log Derivative Trick_**，这个trick的作用是把 $p_{\theta}(x)$ 变出来，使公式能写成期望的形式。现在梯度移到了期望里面，可以用蒙特卡洛法估计了：

$$
\nabla_{\theta} E_{p_{\theta}(x)}[f(x)]
\approx
\frac1m\sum\limits_{i=1}^m f(x_i)\nabla_{\theta} \ln p_{\theta}(x_i) 
$$

$\nabla_{\theta}\ln p_{\theta}(x)$ 称为**_score Function_**，是一个梯度场。这个函数比较重要，扩散模型里面也有。score function的一个性质是期望为0：

$$
E_{p_{\theta}(x)}[\nabla_{\theta}\ln p_{\theta}(x)]
=\nabla_{\theta} E_{p_{\theta}(x)}[1]
=\nabla_{\theta} 1
=0
$$

利用均值为0的特点可以通过一些手段降低梯度估计的方差，从而降低估计难度：

$$
\nabla_{\theta} E_{p_{\theta}(x)}[f(x)-b]
=E_{p_{\theta}(x)}[
f(x)\nabla_{\theta}\ln p_{\theta}(x)-b\nabla_{\theta}\ln p_{\theta}(x)
]
=\nabla_{\theta} E_{p_{\theta}(x)}[f(x)]
$$

其中 $b$ 是一个常数，一般叫它**_baseline_**。上式说明了减去baseline不改变估计的期望，但是可以降低估计的方差，因为可以推出方差是一个关于 $b$ 的函数，当 $b=E_{p_{\theta}(x)}[f(x)]$ 时方差最小，且一定比 $b=0$ 时更小，这里就不给出证明了。

#### 轨迹类策略梯度


理解了SFGE后，轨迹类策略梯度可以直接用SFGE方法求，只要求出 $\tau$ 服从的概率密度函数 $p(\tau)$ 的score function即可。记状态的初始分布的概率密度函数是 $\mu(s)$，有了初始分布我们可以写出轨迹 $\tau=<s_0,a_0,r_1,s_1,a_1,\cdots>$ 的概率密度函数为：

$$
p(\tau) = \mu(s_0)\prod_{t=0}^{T} p(s_{t+1}|s_t,a_t)\pi_{\theta}(a_t|s_t)
$$

对它的对数求导得到score function：


$$
\begin{aligned}
\nabla\ln p(\tau) &= \nabla\ln\mu(s_0) + \nabla\sum_{t=0}^{T} \ln p(s_{t+1}|s_t,a_t) + \nabla\sum_{t=1}^T \ln\pi_{\theta}(a_t|s_t)
\\
&= \nabla\sum_{t=1}^T \ln\pi_{\theta}(a_t|s_t)
\end{aligned}
$$

把这个score function带进SFGE公式就推出了轨迹类策略梯度。

#### REINFORCE算法

REINFORCE算法使用蒙特卡洛法采样轨迹来估计策略梯度，每次采样一条轨迹计算loss更细一次模型，当然采样多个组成batch计算loss也是可以的。对于轨迹 $<s_0,a_0,r_1,\cdots,r_{T-1},s_T>$，用这条轨迹的回报 $G_t=\sum\limits_{s=t+1}^{T-1}{\gamma}^{s-t-1}r_{s}$ 近似 $Q(s_t,a_t)$，于是策略梯度为近似为：

$$
\frac1T\sum\limits_{t=0}^{T-1}
G_t\nabla \ln\pi_{\theta}(a_t|s_t)
=
\nabla\frac1T\sum\limits_{t=0}^{T-1}
G_t\ln\pi_{\theta}(a_t|s_t)
$$

直观来看，这就是以$G_t$为权重的交叉熵损失函数，$G_t$ 越大越鼓励该行为。

前面SFGE方法中说明了通过减去baseline可以减小方差，让 $G_t$ 减去一个常数 $B$ 使得交叉熵的系数有正有负会更好，**只要这个常数 $B$ 跟action无关就行**。引入baseline后公式变为：

$$
\nabla \frac1T\sum\limits_{t=0}^{T-1}
[(G_t-B)\ln\pi_{\theta}(a_t|s_t)]
$$

$B$ 一般使用 $G_t$ 在Batch内均值。减去Baseline的就是鼓励超过平均奖励的行为，不鼓励低于平均奖励的行为。

REINFORCE算法类似于值函数方法中的蒙特卡洛强化学习算法，每个采样一个轨迹才更新一次，做不到每走一步都更新模型，训练比较慢。后续方法都改为了每步都更新，这时就不能用轨迹类策略梯度，需要使用状态类的策略梯度。

### 状态类策略梯度定理证明

求状态类策略梯度需要先求值函数的梯度 $\nabla V(s)$，之后再求 $\nabla E_{s\sim d}[V(s)]$。

#### 1. 求V(s)梯度
我们先推导 $V(s)$ 的梯度：

$$
\begin{aligned}
\nabla V(s)
&= \nabla E_{a\sim \pi_{\theta}(\cdot|s)}[Q(s,a)]
\\
&= \nabla \sum_{a\in\mathcal A} \pi_{\theta}(a|s)Q(s,a)
\\
&= \sum_{a\in\mathcal A} [\nabla \pi_{\theta}(a|s)] Q(s,a) + \sum_{a\in\mathcal A} \pi_{\theta}(a|s) \nabla Q(s,a)
\\
&= E_{a\sim\pi_{\theta}(a|s)}[Q(s,a)\nabla\ln \pi_{\theta}(s,a)] + \sum_{a\in\mathcal A} \pi_{\theta}(a|s) \nabla Q(s,a)
\end{aligned}
$$

先看第一项，最后一行用到了SFGE的思路，记第一项为 $\phi(s)=E_{a\sim\pi_{\theta}(a\|s)}[Q(s,a)\nabla\ln \pi_{\theta}(s,a)]$。然后再看第二项怎么推导：

$$
\begin{aligned}
\nabla V(s)
&= \phi(s) + \sum_{a\in\mathcal A} \pi_{\theta}(a|s) \nabla Q(s,a)
\\
&= \phi(s) + \sum_{a\in\mathcal A} \pi_{\theta}(a|s) \nabla \left[r+\gamma\sum_{s'\in\mathcal S} p(s'|s,a)V(s')\right]
\\
&= \phi(s) + \sum_{s'\in\mathcal S} \textcolor{red}{\sum_{a\in\mathcal A} \gamma p(s'|s,a)\pi_{\theta}(a|s)} \nabla V(s')
\\
&= \phi(s) + \sum_{s'\in\mathcal S} \textcolor{red}{\gamma \rho^{\pi_{\theta}}(s\rightarrow s',1)} \nabla V(s')
\end{aligned}
$$

红色部分引入了状态转移分布 $\textcolor{red}{\rho^{\pi_{\theta}}(s\rightarrow s', t)}$，表示在给定策略下，从状态 $s$ **经过 $t$ 步**转移到 $s'$ 的概率。公式的最后出现了 $\nabla V(s')$，我们递归地把现有的 $\nabla V(s)$ 带入继续推导：

$$
\begin{aligned}
\nabla V(s)
&= \phi(s) + \sum_{s'\in\mathcal S} \gamma\rho^{\pi_{\theta}}(s\rightarrow s',1) \nabla V(s')
\\
&= \phi(s) + \sum_{s'\in\mathcal S} \gamma\rho^{\pi_{\theta}}(s\rightarrow s',1) \left[\phi(s') + \sum_{s''\in\mathcal S} \gamma\rho^{\pi_{\theta}}(s'\rightarrow s'',1)\nabla V(s'')\right]
\\
&= \phi(s) + \sum_{s'\in\mathcal S} \gamma\rho^{\pi_{\theta}}(s\rightarrow s',1)\phi(s') \
           + \sum_{s''\in\mathcal S}
                \textcolor{red}{\sum_{s'\in\mathcal S} \gamma^2\rho^{\pi_{\theta}}(s\rightarrow s',1)\rho^{\pi_{\theta}}(s'\rightarrow s'', 1)}
                \nabla V(s'')
\\
&= \phi(s) + \sum_{s'\in\mathcal S} \gamma\rho^{\pi_{\theta}}(s\rightarrow s',1)\phi(s') \
           + \sum_{s''\in\mathcal S} \textcolor{red}{\gamma^2\rho^{\pi_{\theta}}(s\rightarrow s'',2)}\nabla V(s'')
\\
& \cdots \ \ \ \ \textcolor{gray}{\text{Repeatedly unroll}\ \nabla V(s'')}
\\
&=\sum_{t=0}^{\infty} \sum_{x\in\mathcal S} \gamma^t \rho^{\pi_{\theta}}(s\rightarrow x, t)\phi(x)
\\
&= \sum_{x\in\mathcal S}  \textcolor{red}{\sum_{t=0}^{\infty} \gamma^t \rho^{\pi_{\theta}}(s\rightarrow x, t)}\phi(x)
\\
&= \sum_{x\in\mathcal S}  \textcolor{red}{\frac1{1-\gamma} \eta^{\pi_{\theta}}(x|s)}\phi(x)
\\
&= \frac{1}{1-\gamma}E_{x\sim \eta^{\pi_{\theta}}(\cdot|s)}[\phi(x)]
\end{aligned}
$$

倒数第二行到第三行标红的部分引入了**_折扣状态分布_** $\textcolor{red}{\eta^{\pi_{\theta}}(x\|s)=\frac{1}{Z}\sum_{t=0}^{\infty} \gamma^t \rho^{\pi_{\theta}}(s\rightarrow x, t)}$，表示从状态 $s$ 转移到状态 $x$ 的概率，且多步转移的过程中概率随 $\gamma$ 不断降低。里面的 $Z$ 是归一化因子，保证这个概率密度函数积分为1，它的值是：

$$
Z
=\sum_{x\in\mathcal S}\sum_{t=0}^{\infty} \gamma^t \rho^{\pi_{\theta}}(s\rightarrow x, t)
=\sum_{t=0}^{\infty} \gamma^t \sum_{x\in\mathcal S}  \rho^{\pi_{\theta}}(s\rightarrow x, t)
=\sum_{t=0}^{\infty} \gamma^t
=\frac1{1-\gamma}
$$

把这个结果带进去就得到公式 $(20)$。整个推导过程是比较复杂的，需要用到SFGE方法，递归地unroll操作，并引入状态转移函数 $\rho^{\pi_{\theta}}$ 以及折扣状态分布 $\eta^{\pi_{\theta}}$ 来得到比较直观的结果。最后得到的梯度是：

$$
\begin{aligned}
\nabla V(s)
&= \frac{1}{1-\gamma}E_{x\sim \textcolor{red}{\eta^{\pi_{\theta}}(\cdot|s)}}[\phi(x)]
\\
&\approx \frac{1}{1-\gamma}E_{x\sim \textcolor{red}{\rho^{\pi_{\theta}}(\cdot|s)}}[\phi(x)]
\\
&= \frac{1}{1-\gamma}E_{x\sim \rho^{\pi_{\theta}}(\cdot|s)} \left[
    E_{a\sim\pi_{\theta}(a|s)}[Q(x,a)\nabla\ln \pi_{\theta}(x,a)]
\right]
\end{aligned}
$$

最后得到的式子已经和状态类策略梯度很像了，只是外层期望的状态的分布不一样，做了一个替换。当目标函数只关注初始状态 $s_0$ 的 $V(s_0)$ 时，这个公式就是策略梯度定理。

公式 $(22)$ 标红的地方把折扣状态分布 $\eta^{\pi_{\theta}}$ 替换成了状态转移分布 $\rho^{\pi_{\theta}}$，$\rho^{\pi_{\theta}}(x\|s)$ 表示从状态 $s$ 转移到状态 $x$ 的概率（没有限制转移步数）。**做这个替换是因为折扣状态分布的含义不够直观，不容易对它采样**，所以一般直接使用状态转移分布进行采样，基于当前策略rollout就能自然得到这个分布。这个替换会导致现有算法的策略梯度其实都是有偏的，$\gamma$ 越接近1偏差越小，有论文专门吐槽这个偏差：[Is the Policy Gradient a Gradient?](https://arxiv.org/pdf/1906.07073)。从我个人理解来看，外层期望的不同状态分布对应不同的优化目标，反映我们更看重哪些状态，这个替换只影响了对每个状态的看重程度，相当于每个样本的loss权重稍有不同，对训练神经网络的影响不会很大。

#### 2. 求目标函数梯度

前面推出了 $\nabla V(s)$，也自然得到了 $\mathcal L = V(s_0)$ 的情况。下面考虑关注初始分布 $\mu(s)$ 上的价值 $\mathcal L = E_{s\sim \mu(s)}[V(s_0)]$ 的情况：

$$
\begin{aligned}
\nabla \mathcal L
&= \sum_{s\in \mathcal S} \mu(s)\nabla V(s)
\\
&\approx \sum_{s\in \mathcal S} \mu(s) \left(
            \frac1{1-\gamma} \sum_{x\in\mathcal S} \rho^{\pi_{\theta}}(x|s)\phi(x)
        \right)
\\
&= \frac1{1-\gamma} \sum_{x\in\mathcal S} \textcolor{red}{\sum_{s\in \mathcal S} \mu(s)
              \rho^{\pi_{\theta}}(x|s)} \phi(x)
\\
&= \frac1{1-\gamma} \sum_{x\in\mathcal S} \textcolor{red}{d^{\pi_{\theta}}(s)} \phi(x)
\\
&=
\frac{1}{1-\gamma} E_{s\sim d^{\pi_{\theta}}}\left[ \phi(x) \right]
\\
&\propto E_{s,a\sim \pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla\ln\pi_{\theta}(a|s)]
\end{aligned}
$$

标红的部分把初始分布和状态转移分布合并成了一个分布，用 $d^{\pi_{\theta}}$ 表示。$d^{\pi_{\theta}}$ 易于采样，因为它表示用当前策略从初始状态开始rollout得到的状态分布，只需要用 $\pi_{\theta}$ 随机游走就能采样。

### Actor-Critic算法

Actor-Critic是REINFORCE算法的改进，基于状态类策略梯度算法可以实现每一步都更新模型。该算法专门用一个神经网络 $Q_w$ 来近似Q函数，这个神经网络叫Critic，而策略就叫Actor。

算法流程：
1. 初始化 $Q_{w}$ 和 $\pi_{\theta}$
2. 迭代 $N$ 次：
    - 走一步得到 $s,a,r,s'$
    - 优化值函数 $Q_w$
        - 如果 $s'$ 是结束状态，优化$[r-Q_w(s,a)]^2$，并开启下一个episode
        - 如果 $s'$ 不是结束状态，再走一步 $a'$，优化$[r+\gamma Q_w(s',a')-Q_w(s,a)]^2$，$s\leftarrow s'$，$a\leftarrow a'$

    - 优化策略，即优化 $\mathcal L(\theta)=-Q_w(s,a)\ln\pi_{\theta}(a\|s)$，<span style="color:gray;">注意对 $\theta$ 求梯度不会影响参数 $w$</span>

Actor Critic相较于REINFORCE可以自由控制更新的时间间隔，可以每一步都更新，适用于没有终止状态的任务，但是要训练两个神经网络，调参和收敛更困难。

这里有的实现是交替训练，有的是把两个loss加在一起训练。Actor-Critic有点像GAN，一个网络对另一个网络进行评估，如果无法收敛，可以让每次值函数和策略的优化就多迭代几次。

### Adavantage Actor-Critic算法

[Adavantage Actor-Critic（A2C）](https://arxiv.org/abs/1602.01783)算法主要在Actor-Critic的基础上做了两点改动，第一点是它是分布式的，用多个进程同时采样，在计算loss的时候能组成一个batch训练，数据更多，方差更小。第二点是把 $Q_w(s,a)$ 替换为优势函数 $A(s,a)=Q(s,a)-V(s)\approx r+\gamma V(s')-V(s)$，来起到减小方差的效果。

为什么要使用优势函数？前面说了baseline等于 $E_{p_{\theta}(x)}[f(x)]$ 时方差最小，在这里的场景就是 $E_{a\sim \pi_{\theta}(\cdot\|s)}[Q(s,a)]=V(s)$，也就是把 $V(s)$ 作为baseline是最优的，这意味着用优势函数替换损失函数里的 $Q(s,a)$ 是最优的：

$$
\mathcal L = E_{s,a\sim\pi_{\theta}}[A(s,a)\nabla\ln \pi_{\theta}(a|s)]
$$

因为训练时无法获得准确的优势函数，所以我们需要用一些方法近似优势函数，A2C采取的近似方法是用神经网络拟合 $V_w(s)$ 并用 $r+\gamma V_w(s')-V_w(s)$ 近似优势函数。这用到了时间差分的思想，下面介绍的GAE用TD(n)的思想来更好的估计优势函数，并且引入了一个超参数用来控制估计偏差和方差。

### Generalized Advantage Estimation

优势函数的最常用的估计方式是[Generalized Advantage Estimation（GAE，2015）](https://arxiv.org/abs/1506.02438)，GAE多步时间差分进行估计：

$$
\begin{aligned}
A_t^1
&\approx r_t+\gamma V(s_{t+1})-V(s_t)
=\delta_t
\\
A_t^2&\approx r_t+\gamma r_{t+1}+\gamma^2 V(s_{t+2})-V(s_t)
=\delta_t+\gamma\delta_{t+1}
\\
\cdots
\\
A_t^N&\approx r_t+\gamma r_{t+1}+\cdots+\gamma_{t+N}^N V(s_{t+N})-V(s_t)
=\sum\limits_{k=0}^{N-1} \gamma^{k}\delta_{t+k}
\end{aligned} 
$$

然后把它们的指数加权平均作为优势函数的估计：

$$
\begin{aligned}
A_t 
&\approx
(1-\lambda)(A_t^1+\lambda A_t^2+\cdots+\lambda^{N-1} A_t^N)\\
&=
(1-\lambda)
[
\delta_t(1+\lambda+\cdots+\lambda^N)+
\gamma\delta_{t+1}(\lambda+\cdots+\lambda^N)+
\cdots+
\gamma^{N-1}\delta_{t+N-1}(\lambda^{N-1})
]
\\
&\xlongequal{N\rightarrow \infty}\sum\limits_{n=0}^{\infty}(\gamma\lambda)^n\delta_{t+n}
=\delta_t + (\gamma \lambda)A_{t+1}
\end{aligned}
$$

其中$\lambda\in[0, 1)$是引入的超参数，用于对方差和偏差做权衡，越接近0方差越小，偏差越大。这里最后得到了一个迭代的式子，可以沿轨迹倒序迭代去计算每个状态和action的优势函数。

### Off-Policy A2C算法

前面介绍的策略梯度流派的算法都是On-Policy的，无法使用其它策略探索的数据进行训练，例如像DQN一样使用replay buffer储存更多数据对神经网络进行更充分的训练，因此将策略梯度算法扩展到Off-Policy是很有必要的。假设探索策略为 $\pi'$，引入重要性采样就得到了off-policy的策略梯度：

$$
E_{s,a\sim \pi'} \left[
\frac{\pi_{\theta}(a|s)}{\pi'(a|s)}A(s,a)\nabla\ln\pi_{\theta}(a|s)
\right]
=
\nabla E_{s,a\sim\pi'} \left[
\frac{\pi_{\theta}(a|s)}{\pi'(a|s)}A(s,a)
\right]
$$

这里用Log Derivative Trick把 $\ln\pi_{\theta}(a\|s)\nabla\ln\pi_{\theta}(a\|s)$ 合并成了 $\nabla\pi_{\theta}(a\|s)$，这样可以把梯度移动到期望外面得到损失函数：

$$
\mathcal L(\theta)=E_{s,a\sim\pi'} \left[
\frac{\pi_{\theta}(a|s)}{\pi'(a|s)}A(s,a)
\right]
$$

Off-policy的好处是可以像DQN一样使用以前的数据，只需要记住当时的策略来计算重要性权重即可。
{:.info}

需要注意的是，如果两个策略相差较大，需要进行比较多的采样才能进行收敛；如果不进行足够多的采样，两个策略必须相差不大。

### PPO算法

[Proximal Policy Optimization（PPO，2017）](https://arxiv.org/abs/1707.06347)是一个相对现代和实用的方法，简单且效果好，被广泛应用在机器人、LLM等领域，现在已经三万引用了。

#### PPO的算法思路

PPO算法的大致流程是：
1. 初始化
2. 迭代 $N$ 步
    * 先分布式的让 $M$ 个模型rollout $T$ 步，收集 $MT$ 个数据存到replay buffer
    * 用replay buffer的数据训练模型，训几个epoch

伪代码是：
```python
envs = make_vectoriezed_env(env_name, num_envs=M)  # B是Batch Size
agent = PPO()

s = envs.reset()
for _ in range(update_num):
    buffer.reset()

    # rollout，用 M个进程，分别运行 T步，收集数据
    for _ in range(T):
        a, probs = agent.take_action(s)  # probs are used to compute surrogate objective
        s_, r, terminated = envs.step(a)
        buffer.push(s, a, r, s_, terminated, probs)
        s = s_

    # train，用 buffer里的数据训练模型
    agent.learn(buffer)
```
和之前方法的显著区别是之前的方法基本上走一步，收集一个数据，训一步，而PPO是走很多步，收集很多数据，训很多步。PPO把探索和利用完全分开了，简单实用且灵活，而且和很多trick相性很好，能达到很好的效果，有个ICLR博客[The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)专门介绍PPO实现里的37个trick。

#### PPO的损失函数

PPO为了支持用之前策略 $\pi_{\rm old}$ 收集的数据训练多步，使用了Off-Policy的策略梯度损失。$\pi_{\rm old}$ 表示用的是之前的策略，在训练过程中 $\pi_{\theta}$ 会之间偏离 $\pi_{\rm old}$。在这个基础上，基于[TRPO](https://arxiv.org/abs/1502.05477)的思想，引入了一个约束避免 $\pi_{\theta}$ 偏离原本的策略太远来避免训练崩溃，这是PPO名字中Proximal的由来。TRPO的约束方式很复杂，但PPO很简单，如果偏离太远就不算它的梯度：

$$
\begin{aligned}
r_{\theta} &= \frac{\pi_{\theta}(a|s)}{\pi_{\rm old}(a|s)}
\\
\mathcal L(\theta) &= E_{s,a\sim\pi'}\left[
    \min\left\{
        r_{\theta}A(s,a),
        \textcolor{red}{\text{clip}(r_{\theta}, 1-\epsilon, 1+\epsilon)}A(s,a)
    \right\}
\right]
\end{aligned}
$$

其中 $\epsilon$ 是超参数，通常取0.2，用来控制策略偏离程度，越大则允许偏离越多。优势函数 $A(s,a)$ 使用GAE估计。Loss里面的$\min$ 函数和 $\text{clip}$ 函数的作用如何理解呢？下面分情况讨论：
* 当 $A(s,a)\ge 0$ 时，重要性权重 $r_{\theta}$ 会被限制在 $[0, 1+\epsilon]$，并且在大于 $1+\epsilon$ 时，由于两个策略差异过大，会截断权重导致这个样本没有梯度，**避免继续最大化这个重要性权重，也即避免模型过于乐观**
* 当 $A(s,a)<0$ 时，重要性权重 $r_{\theta}$ 会被限制在 $[1-\epsilon,0]$，并且在小于 $1-\epsilon$ 时，由于两个策略差异过大，会截断权重导致这个样本没有梯度，**避免继续最小化这个重要性权重，也即避免模型过于悲观**

简而言之，PPO只会用重要性权重接近1的样本训练模型，防止模型在正收益的状态和原来相比过于乐观、在负收益的状态过于悲观，进而避免模型和原来有过大差异。PPO其实还提供了一个更简单的版本，用两个策略的KL散度作为loss约束二者差异：

$$
\mathcal L(\theta) = E_{s,a\sim\pi'}\left[
        r_{\theta}(s)A(s,a) - \beta D_{\rm KL}(\pi_{\rm old}(\cdot|s) \|  \pi_{\theta}(\cdot|s))
\right]
$$

一般情况下，用clip的loss效果更好，在一些情况下（例如奖励稀疏，[Hsu et al.(2020)](https://arxiv.org/abs/2009.10897)），使用KL散度约束更好。

PPO的核心部分`agent.learn`的代码实现如下：

```python
# compute GAE based on collected data in buffer
# compute old_probs, which is \pi_{old}(a|s)
s, a, target, old_probs, advantages = prepare_data(buffer)

# train several epochs
for _ in range(epoch_num):
    batches = make_batch(s, a, target, old_probs, advantages)
    for s_batch, a_batch, target_batch, old_probs_batch, adv_batch in batches:
        # value loss
        value_loss = MSELoss(critic(s_batch), target_batch)

        # policy loss
        probs = actor(s_batch)
        ratio = probs.gather(1, a_batch) / old_probs_batch
        cliped_ratio = clip(ratio, min=1-eps, max=1+eps)
        policy_loss = - min(adv_batch * ratio, adv_batch * cliped_ratio).mean()

        # total loss
        loss = value_loss + policy_loss
        loss.backward()
        optimizer.step()
```


## 总结

本文介绍了强化学习里一些基础的算法，包括值函数的近似的方法以及策略梯度方法，其中策略梯度方法首先介绍了不同类型目标函数以及对应的策略梯度定理。从策略梯度定理容易得到两个直接的算法：REINFORCE算法和Actor Critic算法。后续介绍了Actor Critic算法的一些改进，包括如何更好地估计优势函数以及如何把Actor Critic修改为Off Policy以支持更好地探索，最后介绍了成熟且常用的PPO算法。如果理解了这篇博客的内容就算是入门强化学习了，后续可以开始看论文去更深入的理解强化学习了。
