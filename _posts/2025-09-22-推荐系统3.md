---
title: 【推荐系统】推荐系统中关注的话题
tags: 推荐系统 机器学习 深度学习 笔记
---

<!-- 
updates:
* 202508:一边面试，一边写
* 20250922: 开始泛读论文去写
-->


<!--
推荐系统模型很直接，实质上还是在学特征之间比较基础的联系，比如线性回归的加权，LR的baseline本身结果就很好，不需要学习CV或NLP里面那些很high-level的抽象表征。
-->


### 用户体验与业务目标

#### Feedback Loop问题

推荐系统决定用户看到的内容，用户的行为会以训练数据或序列特征的形式影响推荐系统，形成一个闭环。这样会导致马太效应：item本身由于内容差异以及冷启动问题会在热度上形成一个长尾分布，而由于推荐系统feedback loop的特征，被推荐得越多的item用户看到的也越多，导致长尾加剧。论文[Feedback Loop and Bias Amplification in Recommender Systems](https://arxiv.org/abs/2007.13019)对这个现象进行了分析。这种现象可以说是探索不足，利用太多导致的，信息茧房就是这么来的（信息茧房英文是Filter Bubble，气泡变成茧房了哈哈）。

<!-- #### 内容分层

维护精品内容（PGC），改变内容平台在用户心中的定位，提高用户使用时长。

公司内部会根据内容标签，比如游戏、生活等内容会由不同的团队设计模型，针对这样更专门的场景进行优化。在AB测试时，一种常出现的情况是在一些内容上负向，但是在其它内容上正向，这个要根据运营目标抉择。

内容可以分为UGC和PGC，内容是流量平台的基石，运营好平台内容非常重要。PGC是平台内的专业内容，可以吸引专业用户或者带有学习目的的用户；UGC是平台内的非专业内容，可以保证平台内容多样性，增强用户互动性。

内容可以按热门度划分，按类别划分等。热门的内容可能是整活，短期热门，也可能是专业度很高的内容，长期热门。 -->

#### 用户分层

不同用户的兴趣差异非常大，用户使用APP的目的也各不相同，对用户分层的理解有助于推荐系统的宏观决策。用户可以按年龄划分，按使用APP时长划分、按价值划分、按职业、按性别划分。也可以按需求划分，例如有些用户刷b站就图个乐，有的则是会找一些网课、评测这种更专业的内容，分别对应平台内对UGC和PGC内容的维护。

<!-- APP的用户可以分为深度用户和新回用户（新用户、回流用户、未注册用户），深度用户推荐算法不太会影响他们的留存，更多看平均使用时长，而新用户体验不好可能就不会使用这个APP了，看重DAU。 -->

#### 用户生命周期价值

Lift Time Value（LTV）指**_用户生命周期价值_**，用来表示建模用户长期价值。例如，在电商广告中，LTV就是一名用户在使用APP过程中购买商品所花的钱的总和。通过训练模型预测一个用户的LTV显然可以帮助找准用户定位，进行更精确的广告定向、优惠券发放等等。一般来说一个用户还在使用APP是没法获取LTV的只能预测，既然如此如何获取用户训练数据呢？一般是选择那些已经流失的用户的数据作为训练数据。

LTV是一个回归任务，直观来说可以用MSE Loss预测。但是，谷歌在[ZILN](https://arxiv.org/abs/1912.07753)这篇论文指出，很多用户的LTV是0，剩下的部分呈现对数正态分布（log normal），也就是LTV的对数服从正态分布（用变量 $y$ 表示LTV）：

$$
f(y) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\textcolor{red}{\ln y}-\mu)^2}{2\sigma^2}}
$$

为了建模这种特性，ZILN使用MLP预测三个指标：$\mu$，$\sigma$，$p$。其中 $\mu$ 和 $\sigma$ 是均值和方差（方差用softplus激活函数保证为正），可以使用最大似然估计训练；而 $p$ 表示这个用户LTV不为0的概率，可以用交叉熵损失训练。预测这几个指标后，最终预测的用户LTV是 $p\cdot e^{\mu+\frac{\sigma^2}{2}}$，表示用户的期望LTV。

最后，ZILN的训练Loss为（这里 $y$ 是标签，即真实LTV）：

$$

\mathcal L = 
\begin{cases}
-\ln (1-p),\ y=0\\
-\ln p + \ln (y\sqrt{2\pi}\sigma)+\frac{(\ln y-\mu)^2}{2\sigma^2},\ y>0
\end{cases}
$$

前面介绍了如何合理的预测LTV，但是LTV一看就是很难预测的指标，应该根据什么特征预测呢？可以选择用户的性别、年龄、国家、所属人群的特征、使用的手机型号、使用APP的参与度（打开次数、点击次数、使用时长等等）、使用时间的pattern、过去的购买信息等等。LTV的特征工程也是一个复杂的问题。

ZILN是2019年的论文，比较老了，后续也有一些LTV的改进，例如2023年腾讯的[ExpLTV](https://arxiv.org/pdf/2308.12729)。

### 推荐系统的各种Bias

#### Sample Selection Bias

Sample Selection Bias（SSB）指训练样本的分布和推理时见到的样本分布存在差异。例如推荐里面训练样本和item热度高度相关，热度高的训练样本就多，但是推理时又不是这么一回事，二者之间存在gap。比如召回模型推理时总是见到全部的样本，但是训练时很多都是那些热度很高的item，比如那些播放量千万的视频占据了太多训练机会，而大多数视频播放量可能只有几百几千，这还是冷启动扶持过的结果。

召回模型的SSB问题有比较明确的解决方案，例如logQ矫正打压热门物品、自监督学习等等。召回阶段的SSB问题比较明显，但其实CVR模型也有SSB问题：样本由大到小可以分为全量样本、召回样本、曝光样本、点击样本、转化样本，模型用于训练的样本都是click样本，但实际推理的时候会对所有曝光样本进行CVR预测，包括click和非click的，因此存在bias。比较经典的解决方案[ESMM](https://arxiv.org/abs/1804.07931)，通过CTR和CVR协同训练、共享embedding来适应所有曝光样本。实际上，所有模型都有一定的SSB问题，模型在线上总会遇到新的item、新的用户，都是训练集所没见过的。

#### Position Bias

指由于推送内容在app中的位置导致的bias，item展示的位置对用户是否点击有显著影响。比如双栏推荐，用户更容易点击第一个，其余位置的物品，用户没有点击并不意味着他不感兴趣，用户可能根本没注意。

Position bias有很多相关的研究，其中比较简单的解决办法是，在训练的时候位置作为特征输入到模型里面，并且加到模型比较靠后的位置，例如直接加到最后的logit上。在推理的时候把位置特征去掉来排除位置特征的影响。

#### Duration Bias

Duration Bias是视频推荐里的概念，视频推荐需要预测用户观看时长，但长视频的观看时长显然容易更长，长视频相较于短视频有优势，所以存在bias。解决这个问题比较简单好用的方法是2022年快手的[D2Q](https://arxiv.org/abs/2206.06003)，从因果推断的角度分析了视频时长的影响，并给出了一个简单的算法：将视频按不同时长分桶，统计每个桶内观看时长的累积密度函数，把每个样本的标签替换为观看时长在这个累积密度函数分位点。这样就把问题转换为了和视频时长无关的quantile prediction，通过分桶也减小了视频时长对quantile的影响。

#### 其它bias

* Exposure Bias：只有曝光样本知道标签，未曝光的样本标签未知
    >  Learning from the logged feedback is however subject to biases caused by only observing feedback on recommendations selected by the previous versions of the recommender.
* Extreme Bias：只有用户非常喜欢或不喜欢的时候才会进行点赞或收藏等行为
* Conformity Bias：大多数人的评价会影响用户判断

这篇[综述](https://arxiv.org/abs/2010.03240)对推荐系统里的bias做了比较好的归纳和分析。




### 模型改进

#### 高估低估优化

广告系统需要精确的CTR和CVR预估值去竞价。但神经网络预测的概率值通常并不对应真实的概率值，很容易极端地偏向0或1，具体可以参考论文[On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)。解决这个问题的做法一般是校准（Calibration）。

通常模型预估值不准体现在模型预测值高估或低估了，而高估低估的原因主要有2类：

1. 数据流正负样本比例在时间上不稳定，带偏模型，这通常体现在模型在某一时间段高估或低谷。

2. 模型本身的偏差。例如神经网络如果过拟合，会导致模型非常自信，预测的概率总是接近0或接近1；模型在一些场景学得好，但是在一些场景拟合不好，存在明显偏差，通常体现在一些特定场景数据上有高估和低估。

推荐系统中使用PCOC指标评估模型的高估低估程度，PCOC的全称是Predict click over click，表示模型预测的CTR除以真实CTR。如果PCOC大于1表示高估，小于1表示低估。

常见的校准算法是在模型预测值的基础上使用保序回归，还有保序回归平滑校准算法。


<!-- 
https://zhuanlan.zhihu.com/p/142086309。
-->


#### 特征建模

* 连续特征：
    * 保持连续值输入：直接输入，或者经过归一化、取对数、标准化等操作后输入
    * 转化为离散特征：各种分桶方法
    * Soft离散化：根据离散化的思路设计输入神经网络处理连续值，例如[AutoDis(2020)](https://arxiv.org/pdf/2012.08986)
* 离散特征：过Embedding层，Embedding层有很多相关研究
    * Embedding压缩（假设原本embedding大小为n）：
        * 引入淘汰机制：例如只给出现频率高于阈值的添加embedding、LossAwareEvicit、去掉30天没出现的值的embedding
        * 量化压缩：把embedding层降低到半精度
        * 神奇的[Deep Hash Embedding(2020)](https://arxiv.org/pdf/2010.10784)
        * 基于哈希的方法：[Double Hash(2017)](https://arxiv.org/abs/1709.03933)、[Hybrid Hash(2020)](https://arxiv.org/pdf/2007.14523)
    * Embedding层的结构：残差embedding
* 时间特征：时间特征是推荐系统里一个非常重要的特征，无论是绝对时间还是相对时间都很重要
* 多模态特征：RQVAE、RQKMeans


#### 行为序列建模

推荐系统希望建模用户兴趣或意图，判断依据主要有两类：一是基于用户画像，二是用户行为。前者可以说是一些刻板印象，后者则更能反映用户的个性化偏好，一般各种行为特征占了总共特征数量的90%，剩下的才是用户画像、item特征、context特征。

* 长序列建模
    * [TWINv2(2024)](https://arxiv.org/abs/2407.16357): TWIN的改进版本
* 序列推荐的范式
    * [SASRec(2018)](https://arxiv.org/abs/1808.09781)
    * [PinnerFormer(2022)](https://arxiv.org/abs/2205.04507)
    * [HSTU(2024)](https://arxiv.org/abs/2402.17152): 提出了一个更轻量级的模型用于替代transformer，可以用在序列推荐上进行召回，也可以用来排序

#### 交叉模型

* MLP-based:
    * [gated DCN](https://arxiv.org/pdf/2311.04635)
* Transformer-based:
    * [DHEN(2022)](https://arxiv.org/abs/2203.11014)
    * [RankMixer(2025)](https://arxiv.org/abs/2507.15551)

#### 多兴趣建模

* [MIND(2019)](https://arxiv.org/abs/1904.08030)
* [Trinity(2024)](https://arxiv.org/abs/2402.02842)

#### 新范式

* 推荐大模型：[HLLM(2024)](https://arxiv.org/abs/2409.12740)
* 生成式召回：[DSI(2022)](https://arxiv.org/abs/2202.06991)、[TIGER(2023)](https://arxiv.org/pdf/2305.05065)
* 生成式推荐：[OneRec(2025)](https://arxiv.org/pdf/2506.13695)、[OneRecV2(2025)](https://arxiv.org/pdf/2508.20900)

### 其它

#### 强化学习视角下的推荐系统

基于强化学习、考虑长期价值的推荐：[topK off-policy correction(2018)](https://arxiv.org/pdf/1812.02353)。个人感觉这个视角挺有意思，从强化学习的视角看待推荐系统：
* 推荐模型是agent
* 动作空间是item集合，更严格的说是item集合中含有K个元素的子集（选topK去推荐）构成的集合
* 每个user都是一个environment
* 用户行为就是环境的反馈，对应reward
* 状态则是用户的偏好，会随时间变化
    * 可以认为在某一个时刻，用户兴趣是固定的，此时只有一个状态，类似于多臂老虎机是一个One State MDP，不需要关注状态转移。
    * 用户兴趣可以用不同的方式建模，例如用一个隐式的表征建模，这是通常的做法。也可以用user在item上的概率分布表示，或者搞成在codebook上的概率分布表示。

基于强化学习的推荐模型可以认为是在**训练一个在多个环境（user）通用的模型，其中大部分参数是跨环境共享的，少部分参数（user embedding）是独立的**。从强化学习的视角来看，推荐系统是困难的：
* 用户兴趣是随时间变化的，因此是non-stationary的环境，且用户数量会变化
* 动作空间是时变的，因为item数量随时间变化
* 训练是offline的，因为oneline意味着在线上环境进行探索，随意探索会损坏用户体验
* 由于训练是offline的，因此优化也是off-policy的，基于线上模型收集的数据进行训练


#### 推荐系统的多样性

个体多样性、总体多样性。

#### 安全性和鲁棒性

如何避免刷榜、套利等实际存在的问题？

#### Slate Recommendation

一次推荐一组物品，这些物品搭配在一起可以为用户提供更全面和令人满意的体验。例如，在电子商务领域，一个“slate recommendation” 可能包括一组服装物品，以创建时尚的装扮，而不仅仅是一件单一的服装。

<!-- 
#### 线上线下不一致

离线数据训练有提升，但是线上不一定有提升，原因在于：
- 除此之外，在线和离线的实现会有很多差异。
- AUC和业务指标本来就不太一致
- SSB问题。粗排送过来的数据和训练用的曝光过的数据不同
- 线上存在大量新样本，与离线不一致
- 特征差异：
    - match特征存在穿越问题
    - example age的穿越问题

[https://zhuanlan.zhihu.com/p/42521586](https://zhuanlan.zhihu.com/p/42521586)

[https://blog.csdn.net/legendavid/article/details/80653433](https://blog.csdn.net/legendavid/article/details/80653433)


#### 出价
pid -> MPC -> RL
生成式强化学习
-->
