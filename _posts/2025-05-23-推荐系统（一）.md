---
title: 【推荐系统】算法介绍
tags: 推荐系统 深度学习 笔记
published: false
---

## 推荐系统介绍

### 数学描述

User-Item矩阵：每行代表一个用户，每列代表一个物品，矩阵中的每项表示用户对该物品的评分。

- 有时候用户无法对物品进行评分，比如点赞都只能点一个，这个时候就用1和0来表示。这种用户没有直接进行评分叫做隐式反馈，很多隐式反馈间接反映了用户的评分。
- User-Item矩阵最大的特点是非常非常稀疏，大部分项都是空的，因为用户很多很多，物品更多，一个用户只可能与极少数的物品交互过。
- 记${\rm score}(u, s)$是用户$u$对物品$s$的打分（User-Item矩阵中对应的项）

推荐系统就是要预测矩阵中空项的值。

### 推荐系统的链路

推荐系统希望将数以亿计的平台内容筛选出几个推荐给用户。为了达到这个目的，通常分为2个阶段：召回、排序。
召回考虑到大多数内容都是用户不感兴趣的，召回希望能够快速的从数以亿计的item中筛选出用户可能会感兴趣的几千个item。而排序则是对这几千个item按用户感兴趣的程度进行排序。现在排序通常使用深度学习模型，对每个item都预测一个分数，比如点击的概率，依据这个模型预测的分数进行排序。但是这样计算量太大，于是排序被进一步拆分为精排和粗排。粗排使用比较小的模型从几千个item排序选出top300的item交给精排模型，精排模型通常比较复杂，对这300个item进行进一步的排序。

召回的目标是提高召回率，精度可以低；而排序的目的是提高精度，保证单次推荐的精确性。
> 有一个简单的场景能很好地理解上面的观点：一个人看了非常多并且只看了“美女”标签的内容，按道理精排模型肯定把所有“美女”的item前面。那是不是我们全部推荐“美女”的item效果就最好呢？不一定。召回要有多样性，有些类别虽然打分不一定最高，但是也要给机会。然后你就惊喜地发现，喜欢看美女的用户，往往也关心国家大事 。

排序被进一步分为粗排和精排，粗排的作用是用更小的模型预先筛选一部分给精排使用，这是出于计算量考虑的妥协，本质上还是希望得到更精确的排序。粗排迭代的思路是尽量向精排看齐。



粗排通常使用简单的双塔模型，不会使用交叉特征，这样可以预先算好item侧的embedding。

### AB测试

# 召回

## 协同过滤算法

- 协同：根据用户之间的相似度将用户分组

    每一组内的用户偏好类似，可以推荐相同的商品（“猜你喜欢”）。这是基于用户的协同过滤。

- 过滤：根据物品之间的相似度，给用户推荐与用户浏览过的物品相似的物品，过滤不那么相似的物品

    比如很多人都喜欢衣服A和B，可以认为两件衣服类似，如果有人只浏览过A，就可以给他推荐B（“喜欢这件商品的人也喜欢xxx”）。这是基于物品的协同过滤。

用户特征和物品特征分别可以根据User-Item矩阵的行向量与列向量表示，因为User-Item矩阵记录了用户与物品的交互信息，这样的交互信息同时反映了用户的偏好和物品的特性。

用户之间的相似度可以用行向量之间的余弦相似度衡量，物品之间的相似度可以根据列向量之间的余弦相似度衡量。

  当然，别的相似度指标也可以，比如交并比。

### 基于用户的协同过滤（u2u2i）

假设要给用户$u$推荐物品，考虑与$u$最相似的$k$个用户$u_1,\cdots,u_k$，可以通过以下方式估计用户$u$对物品$s$的打分：

$$
{\rm score}(u,s)
=\sum\limits_{i=1}^k {\rm sim}(u, u_i){\rm score}(u_i,s)
$$

其中${\rm sim}(u,u_i)$表示用户$u$与用户$u_i$的相似度。于是只要找出评分最高的几个物品给用户$u$做推荐就可以了。

对部分行加权求和

### 基于物品的协同过滤（u2i2i）

假设要给用户$u$推荐物品，考虑用户$u$交互过的$k$个物品$s_1,\cdots,s_k$，可以通过以下方式估计用户$u$对$s$的打分：

$$
{\rm score}(u,s)
=\sum\limits_{i=1}^k {\rm sim}(s,s_i){\rm score}(u,s_i) 
$$

其中${\rm sim}(s,s_i)$表示物品$s$与$s_i$之间的相似度。有了得分后就可以找出评分最高的几个物品给用户做推荐了。

对部分列加权求和

用一个id表示一条召回支路。

## 多路召回


u2i
- 协同过滤的缺陷
    - ItemCF：小圈子用户错误的导致物品相似，解决这个问题需要识别小圈子。Swing模型会考虑用户重合度，重合度高的用户在计算物品相似度的时候权重低，保证喜欢同一个物品用户有足够的多样性
    - UserCF：需要降低热点物品对相似度的影响

    协同过滤落地需要建立索引，user→item的索引，user→user的索引，item→item的索引，每个索引记录topK相关的物品。
i2i
u2u2i

倒排索引。
- 建立tag倒排索引

非个性化召回
- 热点召回
- 基于新鲜度召回

个性化召回
- 其它
    - 地理位置召回，使用经纬度计算GeoHash
    - 关注列表召回
    - 相似作者召回
    - 缓存召回：把之前精排的item缓存起来，作为一路召回



## 向量召回

向量近似检索ANN。

实现比较复杂，模型存在parameter server里，由于embedding会在线更新，所以需要定期更新索引。
- 基于Embedding的算法
    - 矩阵补充算法
      - 效果不好
          - 没用到用户信息和商品信息
          - 负样本选取不对
          - 内积不如余弦相似度
          - 平方损失不如交叉熵
      - 近似最近邻算法

    - 近似最近邻
      - Faiss，Milvus等
      - 建立索引，实现近似搜索

    - 双塔模型

      召回模型是CLIP那种后融合的，召回要保证提前提取特征，不能用前融合。前融合通常效果更好，用于排序。

      - 除了用户的embedding，还额外引入了用户的特征，物品也类似
      - 把这些特征输入到MLP之后编码，再求相似度
      - 训练有几种方式
          - point-wise，相当于二分类，预测点击概率

              一般正负样本比例1：2或1：3，不知道为什么。。。
          - pair-wise，一个正样本，一个负样本
          - list-wise，一个正样本，多个负样本

              infoNCE loss，类似于对比学习。
      - 正负样本的选取
          - 正样本是用户点击的物品，用triplet loss训练

              正样本通常很热门，为了避免热门样本越来越热门，冷门的越来越冷门，所以会对正样本降采样。
          - 负样本的选取没有那么简单。
              - 没有被召回的是负样本（简单负样本）

                  由于被召回的很少，所以随机从所有物品选就行了。

                  负样本的选取需要打压热门物品，热门物品可以用点击次数衡量，采样的时候可以取0.75次方，适当缩小热门物品和冷门物品的差距。让采样概率$p$正比于 $c^{0.75}$，$c$是点击次数。
              - 被召回但是被排序筛选掉（困难负样本）
              - 曝光了但是未被点击（召回模型不能用这个负样本，因为召回需要保证召回率，但这样提高的是精度，会降低召回率；而且用户可能只是碰巧没点击，没点击不代表不点击）

              通常50%是简单负样本，50%是困难负样本。


      - 线上模型更新

          ![](https://secure2.wostatic.cn/static/wJucPMRtZDBUZtWUHBqKMK/image.png)

          - 每天用新的数据，shuffle打乱，训1个epoch，更新一次模型和embedding
          - 每隔几十分钟，就调整一次用户embedding，做oneline learning

              这个是为了适应用户临时的偏好 

## 召回融合

### snake merge算法

## 曝光过滤

去掉用户看过的物品

布隆过滤器，算是哈希表的一个改进，用来实现类似于集合的功能。优点是占用空间比哈希表少，查询快（保证一定是$O(1)$的复杂度），缺点是只能判断“一定不在集合里”，和“可能在集合里”，是一个偏保守的策略，适用于需要严格过滤的情况。

布隆过滤器无法删除元素，但是计数布隆过滤器可以。


# 排序

排序的依据是什么？

## 特征

### 特征服务

### 常见的特征

- 直接特征：
    - user画像：
        - 用户ID
        - 性别，年龄，注册时间，活跃度，是否是新用户，感兴趣的话题
    - item画像：
        - 物品ID
        - 发布时间，发布地点，笔记内容、tag、字数、视频分辨率等，内容信息量、图片美学、品牌等
    - 统计数据（动态特征）
        - 用户统计特征：用户最近的点赞率、收藏率等，用户最近常看的笔记tag
        - 笔记统计特征：笔记的曝光数、点赞数等等，笔记受众，比如用户性别占比、年龄分布、地域等，笔记的作者特征，比如笔记粉丝数等
    - 场景特征：用户当前的经纬度和城市，当前时刻，早中晚，或者什么节日等
    用户和Item的ID类特征非常重要，但是没有泛化性。

- 交叉特征
- 用户行为特征

不同类型特征的处理：
- 离散特征处理：取embedding
- 连续特征：
    - 分段离散化（分桶）
    - 利用累积密度函数归一化
    - 点击数这种长尾效应严重的，取log(1+x)，或者使用归一化，比如使用点击率

不同的推荐业务特征差异非常大，电商、图文、视频推会有很多独有的特征。

### 特征交叉

#### FM和FFM
- 交叉特征的系数是对称矩阵，矩阵很大，而且很稀疏，FM使用低秩近似来减小了模型参数量

    为什么交叉特征有效？为什么稀疏？可以从例子来看：

    > 同时通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。

- 实现起来也很简单，给每一个特征对应一个N维的embedding，比如$x_i$的embedding记为$v_i$，那么交叉特征$x_{ij}$的系数为$v_i^Tv_j$。
  - 假设一共有$M$个特征，不用低秩分解的话，有$M^2$个参数；如果用低秩分解的话，则只有$MN$个参数，通常$N$远远小于$M$。

  直观理解，这个embedding学到的是特征之间的关联性，越相关的特征会越相似，内积越大。

- FFM
  - FM难以刻画更复杂一些的特征，比如日期
  - FFM也很简单，FM是每个特征学一个embedding，而FFM是学多个embedding，每个embedding称为一个特征域。
  - 每个特征还会赋予一个域的id，第$i$个特征的域ID记为$f_i$。在计算交叉系数时，会根据对方的域取出embedding求内积$v_{i,f_j}^Tv_{j,f_i}$。
  - 引入域的概念后，可以更精细的进行建模。

#### Neural Interaction
两两交叉。

#### 如何确定交叉哪些特征？

self-gated。
特征重要性分析。

## 用户行为建模

### Match特征
用户行为建模的另一类特征是Match特征

用于刻画用户历史行为与item属性等匹配程度。比如用户之前交互了100个商品，这100个商品有10个都是电子产品，现在要给一个电子产品预测点击率，那么match特征就是整数10。这个例子的match是类目的match，也可以是别的match，比如id的match，即用户之前就交互过这个商品，反复的看。

> match特征不为0，说明用户已经和它交互过了，这还要继续推荐吗？答：1. match特征有不同的粒度，有一些粒度是可以泛化的，比如和某个类目的交互次数；2. 广告是跨平台的，可能会在不同平台重复看到一个广告。

### 行为序列建模
行为特征可以更好的反映用户的兴趣，不再只是直接特征和交叉特征的那种比较死板的规则。

用户的lastN特征：之前交互过的N个物品的特征。把它也作为排序模型的输入很有帮助，因为他可以反映用户的**多样**的兴趣，以及兴趣变化，一般用在精排。

实际会根据交互行为的不同，有点击的lastN，点赞的lastN，收藏的lastN等。除此之外，还可以有用户交互过的item的属性列表，比如交互过的商品的类目list，交互过的品牌list等。

对于内容推荐而言，点击是比较浅层的行为，点赞收藏是深层的行为；对于电商而言，从点击→加入购物车→购买是逐渐深层的行为。

- **youTubeDNN（2016）**：首次引入了序列特征，对lastN对特征求pooling得到固定长度的MLP输入
    - [https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf)
    - 在召回阶段，在user侧对用户交互过的item特征做poolling毫无疑问可以让得到的embedding与需要检索的item的embdding更相似
    - 在排序阶段，这种序列特征告诉了模型用户对于候选的item是否有明确的喜好
- **DIN模型（2017）：**把需要排序的物品，和lastN特征求注意力并加权求和得到特征，以此代替简单的取平均。

    DIN的注意力通常是concat之后经过MLP降到一维，而不是transformer那种内积，并且也不经过softmax。

    - DIN认为embedding表示用户兴趣，于是lastN作为一个列表，能够表示用户多样的兴趣
    - 通常DIN使用的lastN特征长度是几十
    - DIN的query特征选择，以及lastN的选择都很有讲究。
        - lastN的序列特征不一定是item的ID特征，也可以是item的各种属性的特征。
        - query的选择会更困难
            - 如果是信息流场景，用户没有明确的意图，可以使用候选item的属性信息concat或加起来
            - 如果是搜索场景，用户意图明确，可以使用搜索用的文本信息
- 模型的进一步发展（用得没有DIN广泛）：
    - **DIEN模型（2018）：**考虑lastN的时间信息，引入RNN+Attention的建模
    - **MIMN模型（2019）**：
    - **SIM模型（2020）：**DIN有偏好短期兴趣的缺陷，如果增大N，计算量会大很多
        - SIM模型增大N，增大到几千
        - 然后选topK来计算注意力
        - topK的选法：
            - 可以根据物品类目直接用规则筛选
            - 近似K近邻（效果好，但是计算量大）
        - 还有，可以引入时间特征
    - **CAN模型（2020）：**把lastN里面的item映射为MLP的参数，再把target Item的embedding输入到这个MLP。
        - 可以说FM就是把这里的MLP退化到线性回归。
        - 工业界应用效果似乎没那么好？

## 排序模型


### 线性回归

### 决策树

由于推荐系统的特征大部分是离散值，学出来的决策树的预测逻辑可以理解为：

```python
if (用户性别==男 && 作者性别 == 女 && 内容类型 == 艺术照 && 作者粉丝数 > 10000):
  pred = 1
if (用户性别==男 && 作者性别 == 男 && 内容类型 == 艺术照 && 机型 == 安卓):
  pred = 0
else:
  pred = 0.5
```

不过现在决策树用得比较少了，现在搜广推里面完全被深度学习模型替代，因为深度学习在保持模型灵活的情况下，训练和部署的方式足够单一，更容易开发通用的系统来迭代模型，同时也使得工业界和学术界的交集变大了。

以前还会用GBDT+LR（2014年Facebook提出来的），用GBDT实现自动化特征工程，用LR进行预测。这里GBDT的特征工程指的是根据特征是否落在某个叶子上决定是1还是0，于是得到一个很长的0和1组成的向量送入LR。不过在深度学习时代，特征工程已经几乎完全被embedding的思想取代了。如果以embedding都视角来看，GBDT+LR实质上也是在学embedding，因为一个特征会被多颗树使用，于是一次预测中，一个特征会对应多个叶子节点中的1，这些1被LR加权，这些权重可以被理解成是这个特征的embedding。

### YoutubeDNN

### Wide & Deep

- wide部分用LR
    - 首先理解交叉特征：它建模了user-item、item-item之间的协同信息
        - 比如(uid, brand)特征，如果是1表示用户买过这个牌子，0表示没买过。如果uid喜欢这个牌子，那么这个牌子的商品点击率应该很高
        - 比如(item1, item2)特征
    - 然后理解特征交叉的粒度：user和item都有属性，也许具有某个属性的user很喜欢具有某个属性的item，那么可以在属性层面进行特征交叉，这样就是一个更细粒度的交叉
        - 比如某个性别的人很喜欢某个品牌的商品
- Deep部分用MLP


## MLP的替代模型

### Deep Cross Network系列

MLP**不善于**学习二次或更高次的函数，于是提出DCN。比MLP效果更好，召回和排序都能用。

* DCNv1
对于DCN的输入$x_0$，第$l$层输入为$x_l$，DCN的计算公式是：

$$
x_{l+1}=x_l+x_0x_l^Tw_l+b
$$

其中$w_l$和$b$是模型参数。可以看出来，和MLP不同的是MLP是参数矩阵对输入进行映射，而DCN是和输入求外积进行交叉，得到矩阵再对模型参数进行映射。这么做其实就是得到了和输入的二次项：

![](https://secure2.wostatic.cn/static/g5SCwuhWGB8DCL5wp2duuD/image.png?auth_key=1747997387-sDqNVjkFrAKTHjiM7eDgZ3-0-7be9e2bcabe05fbc8806c995ffa14861)

如果把公式改成：

```python
x = x + (x_0 @ x^T * W_l).sum(dim=1) + b
```

可能效果还好点，不需要共享参数。


* DCNv2

$$
x_{l+1}=x_l+x_0 \odot (UVx+b)
$$

    UV其实就是低秩矩阵。除此之外，DCN还引入了moe。
* 23年还出了个gated GCN：

$$
x_{l+1}=x_l+x_0 \odot (Wx+b)\odot \sigma(Vx+c)
$$

后半部分其实就是GLU啊。


### 注意力模型

- LHUC
    指给激活只学一个scale，取值范围是[0, 2]，实现方法是加一个可学习的embedding，过sigmoid再乘以2得到scale，把这个scale乘到MLP到激活值上。

- RecycleNet
    

- SENet和Bilinear

    ![](https://secure2.wostatic.cn/static/3QBYNwhCXnPCp99kyk5o3b/image.png?auth_key=1747997388-vtbq1xNVei849wAoc8PYpp-0-92f6be5b3509cabe8548424fb4be000c)

    各种离散embedding特征的交互。

    - SENet实现Field-wise加权
    - Field之间的特征交叉
        - 内积，或element-wise乘法
        - bilinear实现交叉，就是$x^TWy$得到一个特征



## 多任务学习

### Multi-gate MoE

更灵活的实现多指标预测。

为了避免极化，也就是某个专家始终不被使用，会对softmax进行dropout

### 多指标预测

单独预测CTR的话，模型会喜欢推荐标题党视频。

- 加权求和，权重可以用ab测试调
- 视频可以预测预估播放时长
- 也可以根据每个指标的排名进行分数计算
- 对于视频，可以引入看视频的时长

#### 采样纠偏

通常负样本会下采样，这会使得模型预测的点击率等指标相较于实际点击率偏高。虽然对于排序来说只需要相对值，大家一起偏高没有什么问题。但是对于广告推荐而言，这些指标会参与对广告的收费（暂时忽略为什么），所以会有较大的影响，需要纠偏。
纠偏非常简单，只需要调整一下logit就行。使用sigmoid激活函数的纠偏

#### 视频观看时长预测

预测视频的“平均观看时长”的函数而不是CTR
- 训练的时候使用“观看时长加权的交叉墒”
- serving的时候对logit取指数就是平均观看时长

这样相当于有$\sum_i T_i$个正样本，$N-k$ 个负样本，考虑到逻辑回归有：

$$
e^{f(x)}=\frac{p}{1-p}=\frac{\sum_i T_i}{N-k}=E[T]\frac1{1-p}\approx E[T]
$$

因此$e^{f(x)}$ 近似等驾驭预测这个视频的平均预测时长，实际上会略微偏高一点点。

### 评估指标

排序模型基本都是进行二分类的预测，但进行二的目的不是为分类，而是进行排序，因此分类准确率是没有意义的，预测概率的相对大小才是核心，故而大多使用AUC。
