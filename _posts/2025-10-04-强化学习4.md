---
title: 【强化学习】AlphaZero、稀疏奖励
tags: 强化学习 深度学习 笔记
---

之前的博客介绍了强化学习的基础算法，但是把这些算法应用到具体问题上还有一些挑战。本文介绍了强化学习的著名应用AlphaZero和常见的稀疏奖励问题。

## AlphaZero

[AlphaGo](https://www.nature.com/articles/nature16961)以及之后的[AlphaZero](https://arxiv.org/abs/1712.01815)使用蒙特卡洛树搜索（MCTS）+深度强化学习（DRL）实现超越人类的围棋水平。这种结合蒙特卡洛估计和深度学习的思路贯穿了整个AlphaZero算法的设计。并且AlphaGo和AlphaZero的很多设计都考虑了围棋的特点，例如Model-Based、棋盘状态的平移不变性、在CNN的输入中引入围棋的规则等等，非常值得学习。

### MCTS

蒙特卡洛树搜索的思路很简单：通过大量模拟对弈（Rollout）来统计每种下法的胜率，每次选择胜率最高的下法下棋。这有点类似于Alpha-Beta剪枝的搜索算法，只是把节点之间的min-max的关系替换为了期望，统计不同下法的回报（reward函数是赢棋返回1，和棋0，输棋-1）。MCTS的做法相当于rollout非常多次，直接基于rollout的结果用蒙特卡洛法估计值函数：

<div align=center>
<img src="../../../assets/images/posts/2025-10-04/MCTS.png" width="40%" />
</div>

这种rollout是“假想的”，是一种model-based的搜索方法，已知状态会如何迁移才能这样做，在围棋中规则是已知的，棋盘状态也是已知的，所以可以这么做。但是单纯的MCTS在围棋上几乎不可行，因为围棋的状态空间太多了，纯基于搜索的方法几乎不可行。

**一种MCTS的改进方法是把MCTS和一个策略结合**。在每次假想的rollout中，让对方和自己都基于这个策略去下棋，因为这个策略本身就有较强的下棋能力，在下棋的时候就能直接排除很多不好的下法，减小搜索空间，把更多的模拟次数集中在更好地下法上。这样本质上是在估计这个策略的值函数，在估计好后通过argmax选择胜率最高的下法其实是在进行一次策略改进。但是因为整个过程没有反复的改进策略，所以最终效果完全取决于策略本身的好坏，没有不断改进的能力。通常这个策略是使用监督学习或强化学习的方法训练的。

**AlphaGo不仅把策略和MCTS结合，还把V函数用在截断MCTS上，二者都用神经网络近似，并结合REINFORCE和DQN的思路不断训练模型改进策略和值函数估计**。结合价值函数的方法是在搜索的时候，不去把一局下完，而是只下一部分（下到指定步数），剩下的改用 $V(s)$ 估计赢的概率，这样结合了基于搜索的rollout和神经网络的预测，有点类似于时间差分的TD(n)，能够进一步减小搜索的计算量。AlphaGo使用这样的策略下棋收集数据，使用REINFORCE算法训练策略网络，使用DQN训练价值网络，让策略、值函数不断改进，最终达到超越人类的水平。所以AlphaGo算法简单来说就是MCTS+REINFORCE+DQN，细节上则使用了很多方法来加速以及保证它能work。

AlphaZero中实际使用的MCTS是一个改进版本，叫APV-MCTS。在MCTS中，树的每个节点是状态 $s$，每个边则代表在该状态的一个动作，可以表示为 $(s,a)$。APV-MCTS给每个边维护一些属性，包括：
* rollout的次数 $N_r(s,a)$（子树中叶子节点的个数）
* 所有rollout的reward的和 $W_r(s,a)$
* 动作价值函数$ Q(s,a）$
* 先验概率 $p(s,a)$。先验概率是策略函数预测的概率，会给最后的softmax设定比较高的温度系数。

整个搜索分为两个阶段：**前 $L$ 步是第一阶段**，这一阶段忽略策略，每一步在搜索选择行为的时候，会选择 $\text{argmax}_a\{ Q(s,a)+ u(s,a)\}$，其中 $u(s,a)$ 是：

$$
u(s,a) = c_{\text{puct}}p(s,a)\frac{\sqrt{\sum_b N_r(s,b)}}{1+N_r(s,a)}
$$

$u$ 函数用来保证搜索的初期能够进行更多的探索，更偏向先验概率高的行为以及访问次数少的行为。$c_{\text{puct}}$ 是一个常数用来控制探索的程度。**第 $L$ 步到分出输赢是第二阶段**，这一步纯粹基于策略进行rollout。整体来看，这样的两阶段搜索策略是希望搜索的前期进行更多探索，后期棋盘的局势相对确定已定再用神经网络预测，还是相当巧妙的。

注意上面还没有涉及值函数 $V(s)$ 是怎么用的。在rollout的时候，AlphaGo一方面完全基于蒙特卡洛估计维护 $N_r$ 和 $W_r$，同时也会基于值函数去维护 $N_v$ 和 $W_v$。基于蒙特卡洛的估计需要rollout到叶子节点才能统计，而基于值函数的估计只需要rollout到第一阶段结束即可。最后Q函数的估计是 $\frac{W_r(s,a)}{N_r(s,a)}$ 与 $\frac{W_v(s,a)}{N_v(s,a)}$ 的加权求和。训练中的rollout过程有点像神经网络的前向传播，而根据rollout的结果更新每个边上的状态有点类似于反向传播。在推理的时候，下棋的最开始会搜索多步，当对手采取了某个行为后，只需要把对应的子树拿出来继续搜索，这样可以复用之前的大部分搜索结果。

### DRL

策略网络和Value Network都使用了ResNet，其主要考量是围棋规则在是平移不变的，与CNN相契合。模型的输入是 $19\times 19\times C$ 的数组，其中 $19\times 19$ 是棋盘格的大小，而 $C$ 是channel个数。输入策略网络的有48个channel，包含了不同的特征，例如最近几步白棋的位置、黑棋的位置、空白的位置等等。输入Value Network的是49个channel，比策略网络的输入多了一个channel表明当前轮到黑棋还是白棋走。策略网络是一个分类网络，类别数量 $19\times 19$ 。而Value Network是一个回归网络，输出是一个连续值。

两个网络的训练方法相对简单，和之前介绍的强化学习算法类似，只是数据来源不纯粹来源策略网络，而是结合了MCTS的策略收集的。

* 策略网络训练：策略网络使用REINFORCE算法训练，用多个GPU同时下棋收集 $B$ 个Batch进行训练，并使用 $V(s)$ 作为baseline来减小方差。应该说这是一个REINFORCE和Actor Critic的结合算法。因为推理的时候是使用Value Network进行蒙特卡洛树搜索，所以Policy Network的作用是用来训练Value Network，而不是用来推理。这也体现出Model-Based RL的一个特点，能直接搜索总是比模型预测更好。

* Value Network训练：训练数据也是rollout时的数据，会在围棋的不同步数均匀的采样状态 $s$ 进行训练，模型直接预测最终reward，相当于 $\gamma=1$ 的回报。


## 稀疏奖励

稀疏奖励指奖励函数 $r(s,a,s')$ 在大多数情况都是0，只要极少数情况才有非0的反馈。例如使用强化学习训练机器人完成一些操作，只有成功打开门时才获得正奖励，此前的所有移动、抓握等动作奖励都是0。这会导致策略在初始阶段没有任何反馈，不知道怎么做才能完成任务。更严重的是，**如果策略得不到提升，那么它可能永远触及不到能得到反馈的状态，导致策略永远得不到提升**。一个很直接的解决办法是有通过监督学习冷启动地训练一个还不错的策略，跳过初期没有反馈的阶段。但这种方法重心不在稀疏奖励上，所以这里不介绍这种方法，而是围绕如何修改奖励函数、如何改进探索策略以到达有反馈的状态来解决稀疏奖励问题。

### Reward Shaping

Reward Shaping通过人为修改奖励函数，对中间状态引入反馈来解决稀疏奖励问题：

$$
r'(s,a,s') = r + F(s,a,s')
$$

其中 $F$ 是我们人为设计的奖励。那么修改奖励之后是否会导致模型的训练目标发生改变？如何设计 $F$ 才能使最优策略不变？[Policy invariance under reward transformations: Theory and application to reward shaping（1999）](http://luthuli.cs.uiuc.edu/~daf/courses/games/AIpapers/ng99policy.pdf)回答了这个问题，给出了一个 $F$ 需要满足的充分必要条件：$F(s,a,s')=\gamma\Phi(s')-\Phi(s)$，其中 $\Phi(s)$ 是一个 $\mathcal S\rightarrow \mathbb R$ 的函数，叫**_势函数_**。因为它把每个状态映射到一个实数，就像给每个状态设定了一个势能。这种reward shaping方法叫Potential-Based Reward Shaping，替换奖励函数之后，新的V函数和Q函数和原来相比都只相差一个 $\Phi(s)$，因此并不影响 $$\text{argmax}_a$$ 的结果，也就不会影响学到的策略。那么实践中应该如何设计 $\Phi(s)$ 呢？这个有点**类似于A*算法中的启发式函数**，可以用状态之间的距离来表示。例如导航的场景，导航到目标位置就给奖励，否则奖励为0。状态 $s$ 是一个坐标，可以根据坐标计算距离，令 $\Phi(s)=1 - \frac{d(s,s_{\rm target})}{\max d}$，相当于越靠近目标，reward越大。

总的来说，势函数给了一个人为引入先验的方式，可以通过设计势函数引导策略模型去更好的学习。但是需要注意势函数一方面不是那么好设计，另一方面它虽然在理论上很好，但实践中不一定那么容易work，因为实践中都是各种近似，与理论不符合。

### Intrinsic Reward

在稀疏奖励的场景下，模型很难从外部奖励（Extrinsic Reward）获取反馈，为此一些方法设计了**_内部奖励（Intrinsic Reward）_**，通过设计的一些和环境无关的reward驱动模型学习策略，例如设计某种反映“好奇心”或“Novelty”的reward去驱使策略模型学习如何采取行动才能高效地发现新状态，也就是学会探索。这样**一方面能够学到探索这个能力，这种能力对模型理解环境、完成任务有潜在帮助。另一方面，更好的探索能够提高遇到奖励的概率，进而得到反馈**。

#### Curiosity-driven Exploration by Self-supervised Prediction

[Curiosity-driven Exploration by Self-supervised Prediction（2017）](https://arxiv.org/abs/1705.05363)提出了一种名为Instrinsic Curiosity Module（ICM）的方法建模好奇心，将“好奇心”作为一个intrinsic reward驱动模型探索来解决外部奖励稀疏的问题。ICM的思路其实类似于世界模型：预测采取某个行为后未来的状态。之前的方法普遍通过预测图片来做到这一点，但是预测图片很难，所以ICM提出预测未来图片的特征。具体而言，ICM用一个网络 $\phi(s_t)$ 提取状态特征，用MSE Loss监督：

$$
\mathcal L_F = \|\phi(s_t)-\phi(s_{t+1})\|^2
$$

训练数据就是rollout时候的数据，而 $\mathcal L_F$ 本身就用来衡量Curiosity。这样做有两个缺陷，一方面网络很容易崩塌，因为只要特征都是0，loss就是0；另一方，ICM认为**网络 $\phi$ 只应该提取和action有关的特征**，这样做是为了避免状态之间本身存在的差异导致$\mathcal L_F$总是很大，使模型总是在探索。有一个叫“Noisy TV”的思想实验阐述了这个问题：环境中存在一个雪花屏的电视机，雪花噪声是随机的，不影响模型策略，但是它会导致像素差异很大。ICM的解决办法是引入一个额外的Loss：

$$
\begin{aligned}
\hat a_t &= g(\phi(s_t), \phi(s_{t+1}))
\\
\mathcal L_I &= \text{CrossEntropy}(\hat a_t, a_t)
\end{aligned}
$$

其中 $g$ 是另一个神经网络，它根据状态 $s_t$ 和 $s_{t+1}$ 的特征预测采取什么行为才会使状态从 $s_t$ 变为 $s_{t+1}$，通过这种方式约束 $\phi$ 只提取和动作相关的特征。

ICM这篇论文指出，Curiosity的作用不仅仅是驱动探索以获取反馈，还**让模型能够学习目前没用但未来可能有用的能力（通用的能力）**，这体现在即使去掉extrinsic reward只保留Instric reward去训练模型探索超级马里奥的第一关，模型学习到的能力可以让它在第二关更快地通关。

#### Random Network Distillation

[Random Network Distillation（RND, 2018）](https://arxiv.org/abs/1810.12894)是一篇加强探索的论文，其思路是通过建模状态的新颖性，选择更“新颖”的状态探索。

RND把ICM这一类方法归为基于前向动力学预测误差度量新颖性的方法，这类方法的问题在于，预测误差除了来源于模型本身不足以拟合，还来源于状态转移本身的随机性，导致基于这种误差的Instrinsic Reward不能真正反映与决策相关的新颖性。RND提出了一个非常简单的解决方案：不需要根据 $s_t$ 预测未来的状态 $s_{t+1}$，而是用两个网络，输入都是 $s_{t}$，一个是随机初始化的网络，一个是参与训练的网络predictor，它需要做的就是预测随机初始化网络的特征。这种做法的合理性在于，神经网络善于拟合和训练集里类似的东西，对于随机初始化的网络所提取的特征，它本身没有太大意义，但是正好适合用来过拟合地训练predictor。

这种做法能够衡量之前是否见过类似的状态。


### Hindsight Experience Replay

[Hindsight Experience Replay（HER，2017）](https://arxiv.org/abs/1707.01495)是OpenAI提出来的一种方法，用来解决一类特定的RL问题：我们需要训练模型到达某个目标。在每个episode的开始，需要根据任务需要给策略定一个目标状态 $g\in \mathcal S$。策略网络的输入不仅仅是状态 $s$，也包含目标状态 $g$，即 $\pi_{\theta}(s,g)$（因为HER论文里面做的任务都是连续动作空间，所以这里使用确定性策略）。策略网络需要学习如何从状态 $s$ 到达目标状态 $g$。价值函数以及reward函数也都和目标 $g$ 有关。

这种问题一般使用binary reward，即reward是0或1。如果模型能到达目标状态，给奖励1，否则给0，这样的奖励显然是稀疏的。HER的优势是不需要像reward shaping一样精细的设计奖励函数，也就是不需要复杂的reward engineering。

HER的核心思路很简单，在稀疏奖励的场景下，通过rollout得到的一条轨迹 $s_0,a_0,r_1,\cdots,s_T$ 里的所有 $r$ 都是0，意味着模型无法获得训练的反馈。即便如此，我们也能知道通过采取怎样的行为可以到达 $s_T$，因此如果把 $s_T$ 作为目标训练模型就能得到反馈，这种替换目标得到新的replay数据的思路就是HER的核心想法。


HER的思路不同于RND的探索，而是和ICM类似，更充分地利用现有的数据进行学习。

### Reinforcement Learning with Unsupervised Auxiliary Tasks

[Reinforcement Learning with Unsupervised Auxiliary Tasks](https://arxiv.org/abs/1611.05397)

