---
title: 【强化学习】深度强化学习
tags: 强化学习 深度学习 笔记
published: false
---

## 值函数近似

### DQN

已知Bellman最优性方程为：

$$
Q(s,a)=r+\gamma\max\limits_{a'}Q(s',a')
$$

DQN的思想是用神经网络近似$Q$，使得对任意的$ s,a,r,s'$上述方程都能近似成立，即损失函数为为：

$$
\frac1B\sum\limits_{i=1}^B
[r_i+\gamma\max\limits_{a'}Q_{\theta}(s'_i,a')-Q_{\theta}(s_i,a_i)]^2
$$

也就是采样一个batch的$s,a,r,s'$使得方程两边近似相等。

特别地，当$s'$是结束状态时（在$s$采取$a$后如果游戏结束会返回下一个状态$s'$，$s'$是结束状态，但没有实际意义），$Q(s,a)=r$是准确的值（不这样DQN可能不收敛）。综合来看，训练目标为：

$$
target=
\begin{cases}
r & s'是结束状态\\
r+\gamma\max\limits_a Q_{\theta}(s',a) & s'不是结束状态
\end{cases}
 
$$

于是目标函数变为：

$$
J(\theta)=\frac1B\sum\limits_{i=1}^B
[target_i-Q_{\theta}(s_i,a_i)]^2
$$

实际上如果使用连续采样产生的$B$个数据作为一个batch对神经网络训练会导致神经网络严重的过拟合（样本不独立），DQN使用比较大的replay buffer保存走过的$<s,a,r,s'>$（保证样本近似独立同分布，避免过拟合），每次从中采样一个batch的样本来近似期望$E_{\pi}$（replay buffer和batch都是越大越好）。

由于$r+\gamma \max\limits_a Q_{\theta}(s',a)$会随着$\theta$不断变化，这导致训练很不稳定，因此通常会将网络拷贝一份并保持参数不变来构建目标，这个拷贝的网络称为目标网络，记为$Q_{\hat\theta}$，此时训练目标为：

$$

target=
\begin{cases}
r & s'是结束状态\\
r+\gamma\max\limits_a Q_{\hat\theta}(s',a) & s'不是结束状态
\end{cases}

$$

其中$\hat\theta$是目标网络的被固定住的参数，每迭代一定的次数，就令$\hat\theta\leftarrow \theta$。

#### 具体算法

- 初始化$Q_{\theta}$，初始化队列$q$作为replay buffer，拷贝$Q_{\theta}$得到$Q_{\hat\theta}$
- 基于$Q_{\theta}$进行探索（$\epsilon$-greedy），将每次探索的$<s,a,r,s',done>$记录到$q$中，直到到达指定步数
- 迭代$N$次：
    - 从$q$中均匀采样$B$个样本，对每个样本计算$target$

$$
target=
\begin{cases}
r & {\rm if}\ done\\
r+\gamma\max\limits_a Q_{\theta}(s',a) & {\rm if \ not}\ done
\end{cases}
 
$$
    - 计算$J(\theta)$，使用优化算法更新参数
    - 基于$Q_{\theta}$探索一步（$\epsilon$-greedy），将$<s,a,r,s'>$记录到$q$中
    - 每迭代$C$次，$\hat\theta\leftarrow \theta$

#### DQN的优缺点

- DQN是off-policy的，任意的采样策略都是可以的，但是建议使用$\epsilon$-greedy，因为对于复杂的问题状态空间太大，有些状态凭随机策略永远到不了，也就没法把Bellman方程解得特别好；而使用$\epsilon$-greedy可以大大缩小状态空间，因为此时的状态空间是依赖于当前的$Q$的，用缩小的状态空间进行训练能更好的收敛。
- DQN是玄学中的玄学，用神经网络解方程到底靠不靠谱既依赖于问题复杂程度，也依赖于模型的泛化能力。
- 固定target网络以及replay buffer这两个trick很有用。
- 只能解决离散动作空间的问题。

### Double DQN

### Rainbow-DQN

缝合了各种DQN的改进。





## 策略梯度法

策略梯度方法直接用神经网络来近似策略，即$\pi_{\theta}$。

在给定的策略$\pi$下，MDP退化为一个马尔科夫过程，设该马尔科夫过程的平稳分布为$d^{\pi}(s)$，因此可以设定以下目标函数：

$$
J(\pi)
=E_{s\in d^{\pi}}[V^{\pi}(s)]
=\sum\limits_{s\in S}d^{\pi}(s)V^{\pi}(s)
$$

通过最大化$J(\pi)$就能对策略进行优化。对于神经网络近似的策略$\pi_{\theta}$，目标函数可以写为：

$$
J(\theta)=E_{s\in d^{\pi_{\theta}}}[V^{\pi_{\theta}}(s)]=\sum\limits_{s\in S}d^{\pi_{\theta}}(s)V^{\pi_{\theta}}(s)
$$

该目标函数的梯度很难求，但是策略梯度定理指出该函数的梯度可以**只与策略有关**：

$$
\begin{aligned}
\nabla J(\theta)
=&
\nabla \sum\limits_{s\in S}d^{\pi_{\theta}}(s)\sum\limits_{a\in A}Q^{\pi_{\theta}}(s,a)\pi_{\theta}(a|s)
\\
\propto

&\sum\limits_{s\in S}d^{\pi_{\theta}}(s)\sum\limits_{a\in A}Q^{\pi_{\theta}}(s,a)\nabla\pi_{\theta}(a|s)
\\
=&
E_{s,a}[Q^{\pi_{\theta}}(s,a)\nabla\ln\pi_{\theta}(a|s)]
\end{aligned} 
$$

$E_{s,a}$表示在策略$\pi_{\theta}$下$s,a$服从的分布。最后得到的期望可以通过采样来近似，因而使得策略梯度可求。具体来说，通过探索得到轨迹$<s_0,a_0,r_1,s_1,\cdots>$，用轨迹中的$s$和$a$来计算$Q^{\pi_{\theta}}(s,a)\nabla \ln\pi_{\theta}(a|s)$，最后取平均即可。

直观来说$Q^{\pi_{\theta}}(s,a)\nabla \ln\pi_{\theta}(a|s)$就是由$Q$函数加权的交叉熵损失，标签是智能体采取的action，即$Q$比较大就鼓励该action，否则不鼓励该action。

总的来说，在策略梯度法的视角下，$\pi_{\theta}$给出了一个状态$s$的分布，在这个分布下最大化期望奖励就是强化学习的目标，然后我们用SFGE方法求梯度来优化就可以了，比值函数的方法直观很多，也更符合深度学习的框架。

### REINFORCE算法

使用蒙特卡洛法进行采样来求策略梯度。

对于轨迹$<s_0,a_0,r_1,\cdots,r_{T-1},s_T>$，用$G_t=\sum\limits_{s=t+1}^{T-1}{\gamma}^{s-t-1}r_{s}$近似$Q(s_t,a_t)$，于是策略梯度为近似为：

$$

\frac1T\sum\limits_{t=0}^{T-1}
[G_t\nabla \ln\pi_{\theta}(a_t|s_t)]
=
\nabla\frac1T\sum\limits_{t=0}^{T-1}
[G_t\ln\pi_{\theta}(a_t|s_t)]
$$

因此，我们只要将$\nabla\frac1T\sum\limits_{t=0}^{T-1}[-G_t\ln\pi_{\theta}(a_t|s_t)]$作为目标函数然后用梯度下降更新即可。直观来看，这就是以$G_t$为权重的交叉熵损失函数，当$G_t>0$时，则鼓励该行为；当$G_t<0$是，则不鼓励该行为。

上述策略梯度法近似有一个缺陷，即统计量的方差比较大，这可能导致模型难以学到好的策略，比如当奖励总是为正的时候，所有行为都会鼓励智能体，让智能体总是遵照初期受到奖励的策略行动，使得智能体难以探索到具有很大奖励的策略。为此，可以通过让$G_t$减去一个值，不鼓励智能体做哪些奖励不够多的行为。具体来说，公式变为：

$$

\nabla \frac1T\sum\limits_{t=0}^{T-1}
[(G_t-B)\ln\pi_{\theta}(a_t|s_t)]
$$

$B$是$G_s$的均值。这样直观来说，就是鼓励超过平均的行为，不鼓励低于平均的行为。

优缺点：

- 思想非常直观，实现简单
- 策略的学习非常不稳定，可能一会儿学习到好的策略，一会儿又学崩了
- 无法对过去采样的轨迹进行复用，data inefficiency
- 使用蒙特卡洛采样，每个episode才更新一次，训练慢

### 优势函数与GAE

将$Q_w(s,a)$替换为$A(s,a)=Q(s,a)-V(s)\approx r+\gamma V(s')-V(s)$会好很多，$A(s,a)$称为优势函数。优势函数的通常用的估计方式是GAE（Generalized Advantage Estimation），GAE的估计方法如下。

考虑多步时间差分的方法：

$$
\begin{aligned}
A_t^1
&\approx r_t+\gamma V(s_{t+1})-V(s_t)
=\delta_t
\\
A_t^2&\approx r_t+\gamma r_{t+1}+\gamma^2 V(s_{t+2})-V(s_t)
=\delta_t+\gamma\delta_{t+1}
\\
\cdots
\\
A_t^N&\approx r_t+\gamma r_{t+1}+\cdots+\gamma_{t+N}^N V(s_{t+N})-V(s_t)
=\sum\limits_{k=0}^{N-1} \gamma^{k}\delta_{t+k}
\end{aligned} 
$$

然后用指数加权平均作为优势函数的估计：

$$
\begin{aligned}
A_t 
&\approx
(1-\lambda)(A_t^1+\lambda A_t^2+\cdots+\lambda^{N-1} A_t^N)\\
&=
(1-\lambda)
[
\delta_t(1+\lambda+\cdots+\lambda^N)+
\gamma\delta_{t+1}(\lambda+\cdots+\lambda^N)+
\cdots+
\gamma^{N-1}\delta_{t+N-1}(\lambda^{N-1})
]
\\
&\xlongequal{N\rightarrow \infty}\sum\limits_{n=0}^{\infty}(\gamma\lambda)^n\delta_{t+n}
=\delta_t + (\gamma \lambda)A_{t+1}
\end{aligned}

$$

其中$\lambda\in[0, 1)$是引入的超参数，用于对方差和偏差做权衡，越接近0方差越小，偏差越大。实际应用中不可能取无穷项，因此存在偏差。

### Off-policy策略梯度

假设探索策略为$\pi'$，引入重要性采样就得到了off-policy的策略梯度：

$$
E_{s,a\sim \pi'}[
\frac{\pi_{\theta}(a|s)}{\pi'(a|s)}Q^{\pi_{\theta}}(s,a)\nabla\ln\pi_{\theta}(a|s)
]
=
\nabla E_{s,a\sim\pi'}[
\frac{\pi_{\theta}(a|s)}{\pi'(a|s)}Q^{\pi_{\theta}}(s,a)
]
$$

于是损失函数为：

$$
 J(\theta)=E_{s,a\sim\pi'}[
\frac{\pi_{\theta}(a|s)}{\pi'(a|s)}Q^{\pi_{\theta}}(s,a)
]
$$

Off-policy的好处是可以像DQN一样使用以前的数据，只需要记住当时的策略即可，这样可以解决REINFORCE算法的data inefficiency问题。

注意：

- 但如果两个策略相差较大，需要进行比较多的采样才能进行收敛
- 如果不进行多的采样，就要让两个策略相差不大

### Actor-Critic算法

Actor-Critic专门用一个神经网络来近似$Q$函数，使得每步都可以进行更新，当然也可以使用TD(n)。

- 初始化$Q_{w}$和$\pi_{\theta}$
- 迭代$N$次：
    - 走一步得到$s,a,r,s'$
    - 优化值函数$Q_w$
        - 如果$s'$是结束状态，优化$[r-Q_w(s,a)]^2$，并开启下一个episode
        - 如果$s'$不是结束状态，再走一步$a'$，优化$[r+\gamma Q_w(s',a')-Q_w(s,a)]^2$，$s\leftarrow s'$，$a\leftarrow a'$
    - 优化策略，即优化$J(\theta)=-Q_w(s,a)\ln\pi_{\theta}(a|s)$

        注意对$\theta$求梯度不会影响到参数$w$，这完美契合了策略梯度定理

    这里的实现不确定，有的是交替训练，有的是把两个loss加在一起训练。

Actor-Critic有点像GAN，一个网络对另一个网络进行评估，如果无法收敛，每次值函数和策略就多迭代几次

优缺点：

- 可以自由控制更新的时间间隔
- 策略的学习有Critic决定，一旦Critic不准确，策略也会崩
- 要训练两个网络，很难收敛，很难调参
- REINFORCE的缺点它都有

### DDPG

Deep Deterministic Policy Gradient。确定性策略梯度的目标函数为：

$$
$J(\theta)=E_{s\sim d^{\pi_{\theta}}}[Q^{\pi_{\theta}}(s,\pi_{\theta}(s))]$
$$

目标是找到使$Q$最大的策略，由于$Q$未知，DDPG使用DQN的方法近似$Q$。

- 初始化$\pi_{\theta},Q_{w}$，拷贝$\pi_{\theta},Q_{w}$得到target网络$\pi_{\theta'},Q_{w'}$
- 迭代$N$次：
    - 走一步得到$s, a, r, s'$，注意由于是确定性策略，为了能更好的探索，通常会给$a$加上高斯噪声，噪声的方差随着训练次数减小
    - 将$s, a, r, s'$存储到replay buffer中
    - 从replay buffer中采样一个batch的样本，对$\pi_{\theta},Q_w$进行训练
        - 使用DQN损失函数对$Q_w$进行训练，其中$target=r+\gamma Q_{w'}(s',\pi_{\theta'}(s'))$
        - 基于上面的策略梯度目标函数对$\pi_{\theta}$进行训练，注意是梯度上升

        核心就是引入这个$Q$，有点类似于GAN引入判别器间接的训练模型。
    - 使用移动加权平均更新target网络的参数，即：

$$
\theta'=\tau\theta'+(1-\tau)\theta\\
w'=\tau w+(1-\tau)w 
$$

        通常$\tau=0.999$

优点：

- 可以处理连续值的预测

缺点：

- 很难调参，参数调不好很难收敛

### PPO
