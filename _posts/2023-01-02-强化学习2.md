---
title: 【强化学习】Model-Free算法
tags: 随机近似 强化学习 笔记
published: false
---

[https://distill.pub/2019/paths-perspective-on-value-learning/](https://distill.pub/2019/paths-perspective-on-value-learning/)

[https://distill.pub/2020/understanding-rl-vision/](https://distill.pub/2020/understanding-rl-vision/)

## RM算法

### 随机近似——均值估计

设有序列$x_1,\cdots,x_N$，即前$n$项的平均为$\bar x_n$，于是

$$
\begin{aligned}
\bar x_{n+1}
=&\frac1{n+1}\sum\limits_{i=1}^nx_i\\
=&\frac1{n+1}(n\bar x_n+x_{n+1})
=\frac1{n+1}((n+1)\bar x_n-\bar x_n+x_{n+1})\\
=&\bar x_n+\frac1{n+1}(x_{n+1}-\bar x_n) 
\end{aligned} 
$$

这样就得到了一个迭代求平均的方法：

$$
$\bar x_{n+1}=\bar x_n+\frac1{n+1}(x_{n+1}-\bar x_n)$
$$

那么如果把公式改为：

$$
w_{t+1}=w_t-\alpha_t(w_t-x_{t+1})
$$

就变成一个类似于梯度下降的形式，在$\alpha_t$满足什么条件的情况下能估计准确？

### Robbins-Monro算法

RM算法用来求零点，这个和均值估计的关系后面说。

如果要求函数$f(\theta)$的零点，可以用数值计算里的迭代方法来求，但$f(\theta)$是得不到的，只能获得噪声的数据$z(\theta,\epsilon)$，这里$\epsilon$表示随机噪声，$z(\theta,\epsilon)$表示受噪声影响。通常以下关系成立：

$$
f(\theta)=E_{\epsilon}[z(\theta,\epsilon)]
$$

$z$其实就是带噪声的$f$，意思是我们只能获取$f(\theta)$带噪声的函数值，无法获取准确值。

记$\theta^*$满足$f(\theta^*)=0$，Robbins-Monro算法通过以下公式对$\theta$进行迭代进行求解：

$$
\theta_{t+1}=\theta_t-\alpha_tz_t
$$

其中$z_t\sim p(\cdot|\theta_t)$。也就是在一定条件下，从$\theta_0$开始，根据$\theta_t$采样得到$z_t$，然后该公式迭代就能让$\theta$收敛于$\theta^*$。这个其实也很好记：如果$z>0$，由于$f$单调，就该让$\theta$小一点才能接近0；如果$z<0$，就该减去$z$让$\theta$大一点。总之就是反着来，前面再加1个学习率。

![](https://secure2.wostatic.cn/static/24mRrEEtswX2Z1Tuquooit/image.png?auth_key=1752317566-trGFkdk9bbFPhy4uLbjgiH-0-a33d2f89fdd9ce6eb7df42cc8e7d692f)

这里的一定条件指：

- $z(\theta,\epsilon)$有界
- $f(\theta)$单调递增
- $\theta^*$存在且$f(\theta^*)>0$
- $\alpha_t$需要满足：

$$
\begin{aligned}
\sum\limits_{t=1}^{\infty}\alpha_t &= \infty\\
\sum\limits_{t=1}^{\infty}\alpha^2_t &= 0
\end{aligned} 
$$
    - 一阶和无穷大，保证了步长不要收敛太快
    - 二阶和有限，保证了步长足够小，最终收敛（常量步长不满足）

该问题在机器学习中挺常见，比如：

- 如果$\theta$是自变量，$f(\theta)$是某个函数的梯度，$A=0$，那么该问题就是一个优化问题，找到梯度为0的点。

    此时$z(\theta,\epsilon)$是mini-batch梯度，于是该算法变为SGD。
- 如果要估计$X$的期望，令$f(\theta)=\theta-E[X]$即可
- 对于value-based强化学习，我们要估计所有状态的期望。对于一个状态而言，我们使用RM算法进行更新

### RM算法估计均值

把均值估计转换为零点估计问题，令$f(\theta)=\theta-E[X]$即可，每次观测到一个$x_t$，那么$z_t=\theta_t-x_t$。

### 随机梯度下降



## Model-Free强化学习

马尔科夫决策过程是四元组$<S,A,P,R>$，实际应用中我们并不知道具体的$P$和$R$，这意味着：

- 我们不知道采取不同的行为会变化到什么状态，
- 我们也不知道在某个状态采取某个行为能得到什么样的reward

不知道$P$和$R$，也就无法使用价值迭代来求解最优策略，因此引出了Model Free的方法。

还有一点需要注意的是，前面我们通过迭代求解$V$函数，然后根据$P$计算$Q$，最后得到策略。而在不知道$P$的情况下，我们没法由$V$计算得到$Q$，因此必须直接求解$Q$（当然后面也有算法需要用到$V$）。

### 蒙特卡洛法

蒙特卡洛法用于估计价值函数$Q(s,a)$，也就是用于策略迭代的策略估计这一步，还需要与策略改进结合才是完整的强化学习算法。

- 轨迹：依照一定策略，从**起始状态**开始到**终止状态**经历的状态、动作、奖励的序列，通常记为$\tau$

    比如$\tau=<s_0,a_0,r_1, s_1,s_1,a_1,r_2,..., s_{T-1},a_{T-1},r_T,s_T>$，注意这里**要求存在终止状态**，比如游戏、围棋之类的要能够结束。
- 蒙特卡洛方法就是用平均值近似期望值，在这里就是求出很多轨迹的回报，然后取平均来估计价值函数（并且使用顺序估计求平均）：
    - 初始化$Q(s,a)=0$，$\pi(s,a)=1/|A(s)|$，$n(s,a)=0$
    - 迭代$N$次：
        - 根据策略$\pi$得到一条轨迹$\tau=<s_0,a_0,r_1, s_1,s_1,a_1,r_2,..., s_{T-1},a_{T-1},r_T,s_T>$
        - $G\leftarrow 0$
        - 遍历$t = T-1,T-2, \cdots,0$
            - $G\leftarrow \gamma G+r_{t+1}$
            - $s\leftarrow s_t, a\leftarrow a_t$
            - $n(s,a)\leftarrow n(s,a)+1$
            - $Q(s,a)←Q(s,a)+\frac1{n(s,a)}[G-Q(s,a)]$
- 如果使用RM算法估计均值，公式也可以改为：$Q(s,a)←Q(s,a)+\alpha[G-Q(s,a)]$，$\alpha$是学习率。

    在强化学习中，环境通常是动态的（非平稳的），值函数的真实值可能会随着策略的改进而变化。如果学习率逐渐减小，算法可能会过早地“冻结”值函数估计，无法适应环境的变化。固定学习率可以保持一定的灵活性，使算法能够持续适应环境。

    虽然固定学习率会导致无法收敛，但是只要比较小就会有一个整体收敛的趋势，只是最后会震荡，类似于随机梯度下降。

#### 同策略与异策略

在model-free的算法中，需要探索环境以及学习一个策略，由于我们希望学习到的策略是确定性的，但为了探索环境又需要随机性的策略，为此可以使用不同的策略。

- 同策略（On-policy）：探索环境与学习的策略是同一个策略（根据自己的经验学习）
- 异策略（Off-policy）：探索环境与学习的策略不是同一个策略（看别人的经验学习）

为确定性策略$\argmax\limits_aQ(s,a)$引入随机性常见的办法是$\epsilon$-greedy:

$$
\pi(a|s)=\begin{cases}
\frac{\epsilon}{|A(s)|} + 1 - \epsilon & a=\argmax\limits_a Q(s,a)\\
\frac{\epsilon}{|A(s)|} & a\ne\argmax\limits_a Q(s,a)
\end{cases} 
$$

用softmax也是常见的操作。

Q-Learning直接求解Bellman最优性方程的好处是，只需要有足够多的(s, a)即可求解最优策略，不需要进行策略评估，因此可以用探索性强的策略去搜索足够多的(s, a)来求解最优策略。

#### 同策略蒙特卡洛强化学习算法

- 初始化$Q(s,a)=0$，$n(s,a)=0$，$\pi(a|s)=1/|A(s)|$
- 迭代$N$次：
    - 根据策略$\pi$得到一条轨迹$\tau=<s_0,a_0,r_1, s_1,s_1,a_1,r_2,..., s_{T-1},a_{T-1},r_T,s_T>$
    - $G\leftarrow 0$
    - 遍历$t = T-1,T-2, \cdots,0$
        - $G\leftarrow \gamma G+r_{t+1}$
        - $s\leftarrow s_t, a\leftarrow a_t$
        - $n(s,a)\leftarrow n(s,a)+1$
        - $Q(s,a)←Q(s,a)+\frac1{n(s,a)}[G-Q(s,a)]$
    - 改进策略：

$$
\pi(a|s)=\begin{cases}
\frac{\epsilon}{|A(s)|} + 1 - \epsilon & a=\argmax\limits_a Q(s,a)\\
\frac{\epsilon}{|A(s)|} & a\ne\argmax\limits_a Q(s,a)
\end{cases} 
$$

#### Generalized Policy Iteration

为什么可以单episode估计策略之后更新policy？

#### exploring starts

#### soft policy

balance exploration(try to see more state) and exploitation(try to get more reward)

![](https://secure2.wostatic.cn/static/aKBtVs1b7TV7xWfku5wbfT/image.png?auth_key=1752316641-d6JWULAvqoHhgLMWQcykrv-0-5834f5b534974f24022600a8160509ee)

epsilon-greedy的一致性：

![](https://secure2.wostatic.cn/static/e96h5jPD9zPJGY7tjw1pfj/image.png?auth_key=1752316641-e3P826BQpyAF8iFbg357d3-0-25e2fb0d41b4e8b886d4215e3dfd2c2a)

在epsilon比较小的情况下，这使得最优的epsilon-greedy可以转成greedy的策略，且最优。

### 时间差分算法

Time difference（TD）learning。

蒙特卡洛方法收敛太慢，因为每次要经过一个轨迹才能更新一次，而时间差分每一步都进行迭代：

- 考虑轨迹$<s, a, r, s', a',\cdots>$
- 时间差分使用$r+\gamma V(s')$来近似采样得到的$G$

    由于估计的过程中用到了$V$自身，因此说TD是bootstrap的。
- TD的更新公式：$V(s)\leftarrow V(s)-\alpha[V(s) - (r + \gamma V(s'))]$
    - $r+\gamma V(s')$称为TD target
    - $r+\gamma V(s') - V(s)$称为TD error，即期望的值与实际值的差异
- 时间差分和MC的直观的区别是什么？从公式上看，唯一的区别是一个用$G$，一个用$r+\gamma V(s')$。这个差异的具体含义可以从一个例子来理解

    [https://distill.pub/2019/paths-perspective-on-value-learning/](https://distill.pub/2019/paths-perspective-on-value-learning/)

    ![](https://secure2.wostatic.cn/static/h9gEjKtjVK69b6ncpqvEg8/image.png?auth_key=1752316641-fSE83stV394CrBFc8gjfHG-0-c8e471e79480e0ce098913dc9571effb)

    - 假如模型第一次走完了一条轨迹，更新轨迹上的$V$，这个时候TD和MC没有区别
    - 在走第二条轨迹的时候
        - 如果轨迹和第一条没有交集，那么TD和MC也没有区别
        - 假如第二条轨迹的一个状态$s'$和第一条轨迹重合了，那么在它的前置状态$s$的更新上，MC只考虑了未来的两种情况，因为只走了2条轨迹；而TD考虑了更多种情况，因为它用了$r+\gamma V(s')$，因为$V(s')$包含了未来的不同情况。
- n-step TD：TD是每走一步就更新，如果愿意多走几步就可以更准确的估计$G$，也就得到了n步TD（TD(n)）方法
    - 考虑轨迹$<s_0,a_0,r_1,s_1,a_1,r_2,\cdots>$
    - 用$r_0 + \gamma r_1 + \cdots +\gamma^n r_n+\gamma^{n+1}Q(s_{n+1},a_{n+1})$估计$G$
    - 迭代公式：$V(s)←V(s)+\alpha[r_0 + \gamma r_1 + \cdots +\gamma^{n-1} r_{n-1}+\gamma^{n}V(s_{n})-V(s)]$

#### MC与TD的对比

- TD是每一步都能更新$v$；MC必须经过一条轨迹才能更新，二者的折中是TD(n)
- TD是高偏差低方差的（前期会带来偏差）；MC是无偏高方差的，TD(n)是二者的折中
- TD不需要能终止；MC必须要求能终止

#### TD为什么比MC更高效

路径合并。

### Q函数估计

#### Sarsa

TD Learning估计V，但是Sarsa估计Q。

- 初始化$Q(s,a)=0$
- 迭代$N$次：
    - 更新策略$\pi$为$\argmax\limits_a Q(s,a)$的$\epsilon$-greedy策略

        策略更新
    - 在状态$s$根据策略$\pi$采取行为$a$，到达状态$s'$，获得奖励$r$，然后再采取行为$a'$，到达状态$s'$

        探索
    - $Q(s,a)\leftarrow Q(s,a)-\alpha[Q(s,a)-[r+\gamma Q(s',a')]]$

        策略估计。
    - $s←s'$
- 得到策略$\pi(s)\leftarrow \argmax\limits_a Q(s,a)$

Sarsa用了$r+\gamma Q(s',a')$作为$G$的估计，这是不对的，正确的是$r+\gamma V(s')$，但是这样又需要估计$V$。我们进一步分析使用$Q(s',a')$代替$\gamma V(s')$会导致什么？首先，这肯定是对value的有偏估计，那么具体是怎么偏差呢？注意到$V(s')$其实是$Q(s',a)$的期望，Sarsa本质上是修改了这个期望，使用模型实际使用过的动作$a'$作为估计，模型如果在$s'$采取不同的动作，会导致估计出不同的$Q$函数。

![](https://secure2.wostatic.cn/static/gJnsPA6UUCHBvfbvXyFWhQ/image.png?auth_key=1752316641-sxT7HjB9FJMdEpVNiHffLw-0-c75767efceda7b14e85370701b544ca5)

这里写得不对。

#### Expected Sarsa

- 初始化$Q(s,a)=0$
- 迭代$N$次：
    - 更新策略$\pi$
    - 在状态$s$采取行为$a$，到达状态$s'$，获得奖励$r$

        不需要再采取动作$a'$了。
    - $Q(s,a)\leftarrow Q(s,a)-\alpha[Q(s,a)-[r+\gamma E_{a\sim \pi(\cdot|s')}[Q(s',a)]]]$

        期望方差更小。
    - $s←s'$
- 得到策略$\pi(s)\leftarrow \argmax\limits_a Q(s,a)$

和sarsa的区别是它本质上在求解：

$$
Q(s,a)=E_{s'\sim p(\cdot |s)}[r+\gamma V(s')]
$$

#### n-Step Sarsa

求解：

$$
Q(s,a)=E[r_t+\gamma r_{t+1}+\cdots+\gamma^n Q(s_{t+n},a_{t+n}) ]
$$

要采样多步才能更新一次。

#### Q-learning算法

最优策略的$Q$函数满足：

$$
Q(s,a)=E_{s'\sim p(\cdot |s)}[r+\gamma \argmax\limits_a Q(s',a)]
$$

如果直接用RM算法估计这个均值，就可以得到Q-Learning方法：

- 初始化$Q(s,a)=0$
- 迭代$N$次：
    - 更新$\pi$为$\argmax\limits_a Q(s',a)$的$\epsilon$-greedy策略
    - 在状态$s$根据策略$\pi$采取行为$a$到达状态$s'$，获得奖励$r$
    - $Q(s,a)\leftarrow Q(s,a)+\alpha[r+\gamma \max\limits_aQ(s',a)-Q(s,a)]$
    - $s\leftarrow s'$
- 得到策略$\pi(s)\leftarrow \argmax\limits_a Q(s,a)$

Q-Learning学到的策略比Sarsa更大胆，原因在于，在回报有噪声的情况下，每次取$\max_a Q(s',a)$会让$Q$的估计过于乐观，偏高，Q-Learning的估计是有偏的，但是这样收敛更快。

如果用别的策略收集了一大堆数据，再用QLearning求解$Q(s,a)$，就是off policy的策略。因为QLearning不涉及策略改进，直接求解的是Bellman最优性方程，因此离线的数据也能训练。

## 总结

从动态规划到Q-learnig涉及到的思想：

- 策略评估只迭代一次
- 每一步采样都进行更新，使用bootstrap的方式来估计G
- 进一步到函数估计，也用了bootstrap的方法

