---
title: 【备忘】LLM部署
tags: 大模型 备忘 配置
---

记录transformers和vLLM的一些使用。
<!--more-->

#### transformers的设置
```python
import os
os.environ['TRANSFORMERS_OFFLINE']="1"  # 让transformers load模型的时候从本地load，不要连接线上避免http timeout
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # 使用hf-mirror下载模型
```


#### 输出格式控制

```python
from typing import Literal
from pydantic import BaseModel, create_model

class Judge(BaseModel):
    rationale: str
    judge: Literal['Yes', 'No']

definitions = dict()
for t in topk_tags:
    definitions[t] = Judge
Results = create_model("Results", **definitions)
LLMOutput = create_model("LLMOutput", description=str, results=Results)


chat_response = self.client.chat.completions.create(
    model="Qwen/Qwen2.5-VL-3B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant. Please answer in Chinese."},
        {
            "role": "user",
            "content": [
                content,
                {"type": "text", "text": prompt},
            ],
        }
    ],
    extra_body={
        "guided_json": LLMOutput.model_json_schema(),
        "guided_whitespace_pattern": "",
        "top_k": 1,  # 确定性输出
    },
)
```


#### 如何减轻大模型幻觉
* 在上下文提供明确的信息
    * 让大模型搜索，或者调用工具（例如编程），这是RAG的思路
    * 可以通过prompt调优实现，拆解问题，让大模型先回答拆解的问题，再回答最终问题
* Chain of Thought，一方面提供思考的demo，另一方面让大模型给出决策的时候提供rationale
* 对于MLLM，可以增大图像分辨率，使用更详细、精确的描述进行监督


#### 大模型应用
* RAG：大模型自己规划任务如何完成
* workflow：人为编排任务，通常会使用节点编辑器实现编排
