---
title: LLM部署备忘
tags: 备忘 配置
---

记录transformers和vLLM的一些使用。
<!--more-->

```python
import os
os.environ['TRANSFORMERS_OFFLINE']="1"
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

#### 输出格式控制

```python
from typing import Literal
from pydantic import BaseModel, create_model

class Judge(BaseModel):
    rationale: str
    judge: Literal['Yes', 'No']

definitions = dict()
for t in topk_tags:
    definitions[t] = Judge
Results = create_model("Results", **definitions)
LLMOutput = create_model("LLMOutput", description=str, results=Results)


chat_response = self.client.chat.completions.create(
    model="Qwen/Qwen2.5-VL-3B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant. Please answer in Chinese."},
        {
            "role": "user",
            "content": [
                content,
                {"type": "text", "text": prompt},
            ],
        }
    ],
    extra_body={
        "guided_json": LLMOutput.model_json_schema(),
        "guided_whitespace_pattern": "",
        "top_k": 1,  # 确定性输出
    },
)
```
