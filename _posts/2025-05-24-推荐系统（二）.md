---
title: 【推荐系统】流式训练
tags: 推荐系统 深度学习 笔记
published: false
---


# 数据流

## 用户日志

用户的所有行为都会以日志等形式写入kafka，之后根据日志类型写入不同的队列，比如点赞、点踩是一类，用户展现日志等等。用户日志非常庞杂，涉及很多数据库，时不时为了数据分析就会用SQL产生一些新表，但是统一都叫数据源。

模型用到的特征来源于数据源，需要经过比较复杂的处理流程变成Key-value服务存储到线上，比如key是user id、item id，然后value是对应user或item到一大堆特征，也是key-value。这里的处理流程通常是访问数据源获取相应的数据（比如年龄等特征），以及一些聚合操作（访问过的item序列）等等。

## 实时特征与批式特征

批式特征指定定期更新的特征，通常是每天凌晨出发对应的任务，对数据源进行处理并更新到key-value服务上。实时特征则是每来一个数据就变一下。通常来说，批式特征是用户画像、长期行为序列一类的东西，实时特征是用户短期的行为序列、地理位置这一类。

- 批式特征在进入数据库后进行处理，每天凌晨12点定时开始提取，由Hive处理变成Key-Value服务
- 实时特征则是在进入数据库前还会走另一条Flink支路，由Flink处理后直接KV化。

Flink和Hive的处理可以由SQL统一，实现流批一体的特征处理平台。

## 训练样本

训练样本来源于请求。用户的每个请求都会触发召回、排序的流程，在这些流程中一方面模型会进行预测，另一方面会把它作为样本存起来。注意此时的样本是没有标签的，因为这些item都还没发送出去。之后等标签回流之后（用户点击、广告主回传的转化也被存进数据库里），这些样本和标签会拼接成训练样本（类似于SQL到join操作，根据请求的id等方法把二者对应起来），经过消息队列给模型训练。

一个样本通常带有非常多的属性（其实就是user和item的特征之类的，会有几万个，会被模型使用的有几千个），这样后续就可以很方便的分析模型在不同样本上的表现，比如不同年龄段样本的表现、某类item上的表现等等。

样本有一个很重要的属性是这个请求对应的时间，这些样本会按这个时间存起来，方便后续模型按天训练。

## 数据采样

上述介绍的训练样本用于训练模型，同样分为批式训练和实时训练。通常每天进行一次批式训练，实时的样本达到一定batch后进行一次在线更新。对于实时训练，由于样本是排队按顺序获得的，这样的样本不像离线训练会shuffle，标签可能不均匀，导致训练不稳定。

### Fast Emit

fast emit会对纠偏造成影响。

- fast emit存在伪负例样本
- 一条样本的label是对其概率分布的采样，不用过分在意“伪负例”问题

### DJW

### 正例打散

# 在线优化算法

### OGD算法

其实就是SGD算法的特例：

$$
w_{t+1}=w_t-\eta_tg_t
$$

其中$\eta_t = 1/\sqrt{t}$。OGD在准确率上表现正常，但是在sparsity上表现不佳，即使加上了 L1 正则也很难使大量的参数变零。一个原因是浮点运算很难让最后的参数出现绝对零值；另一个原因是不同于批处理模式，online 场景下每次$w$的更新并不是沿着全局梯度进行下降，而是沿着某个样本的产生的梯度方向进行下降，整个寻优过程变得像是一个“随机” 查找的过程，这样 online 最优化求解即使采用L1正则化的方式， 也很难产生稀疏解。正因为 OGD 存在这样的问题，**FTRL 才致力于在准确率不降低的前提下提高稀疏性。**

### FTRL算法

[https://zhuanlan.zhihu.com/p/494715594](https://zhuanlan.zhihu.com/p/494715594)

FTRL算法希望学到的参数尽可能稀疏。FTRL引入了累计梯度，这样可以避免在线学习随机性大的问题，同时引入正则化项保证稀疏性：

$$
w_{t+1}=\argmin_w (g_{1:t}^Tw+\frac12\sum_{s=1}^t\|w-w_s\|^2+\lambda_1 \|w\|_1+\frac12\lambda_2\|w\|^2)
$$

后面就看不懂了。。。以后再看吧。

### AdaMom算法

![](https://secure2.wostatic.cn/static/3i5g9fEaZTvFHphmbzGazB/image.png?auth_key=1747997242-t51u3g6d1Qhp8Y2aJtwdTJ-0-9cb25321cce33dcd1132fd5bdb86edf7)



字节跳动在推荐场景使用这个AdaMom的算法，相较于Adam，主要改动在于：二阶动量的移动平均改为上面的直接累加然后除以$c_t$，也就是取平均。使得近期梯度占比更大，更适合在线训练场景。同时学习率会取很小，5e-6，batch大小1024。

除此之外，字节还加上的Delay Compensation，[https://arxiv.org/pdf/1609.08326](https://arxiv.org/pdf/1609.08326)，这是针对parameter server异步梯度下降的一个补偿机制。第$t$时刻的梯度$g_t$可能会被用来更新$\theta_{t+\tau}$，但是正确的梯度是$g_{t+\tau}$，于是用泰勒展开可以进行近似：

$$
g_{t+\tau}\approx g_t+\nabla g_t(\theta_{t+\tau}-\theta_t)
$$

这里\nabla g_t是海森矩阵，可以用下面的方法近似它：

1. $g_tg_t^T$是它的一个无偏近似（$g_t$本身由于SGD的随机原因，是随机变量）
2. $\lambda g_tg_t^T$（$\lambda$是超参数，0到1之间），可以用来trade-off偏差和方差（显然越小偏差越大，方差越小）
3. 对角化trick：假设海森矩阵的非对角线元素都是0，实验效果也很好

于是把$g_t$变成：

$$
g_t+\lambda g_t\odot g_t\odot(\theta_{t+\tau}-\theta_t)
$$

就是delay compensation。

## 标签回流延迟

offline的CVR模型训练采用“种子+增量”的形式，即：在日期为t-1时，模型在前一天种子的基础上，学习t-8的样本（样本已经完全回流），得到种子模型M，在日期为t时，load种子模型M，学习[t-7,t-1]的样本（最近7天的样本，样本没有完全回流），学完后推送上线进行服务。

模型如果直接学习最近7天的样本，将会导致模型低估，有一些解决办法：

- 采用拆塔学习的方案，**塔h_i代表的物理含义是当前样本在回流第i天的转化率**，线上预估时，使用7个head的打分相加。
- 

## 老汤模型

老模型训练太久，用到的数据远远多于新模型，导致新模型很难超过老模型。

一种解决办法是warm up，load以前的权重，但是这样限制了模型不能改动太大，否则load的参数作用没有那么大

每隔一段时间，重新训练模型。

# One-Epoch现象

搜广推模型一般只训练一个epoch，从第二个epoch开始模型performance就会下降。有研究表明，第二个epoch的embedding梯度很小，基本就不会更新了，而MLP还会继续更新。个人的理解是稀疏特征的embedding对输入的表示能力太强、太容易过拟合了，所以MLP继续更新后很容易过拟合，导致performance下降。

One-Epoch也有个好处，就是使用流式训练不需要担心由于没有对样本过很多次导致性能损失，同时训练和测试可以一起做了，不需要划分训练集和测试集，因为反正模型也只见这个样本一次。